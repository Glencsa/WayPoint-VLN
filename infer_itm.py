import os 
import torch
import torch.nn as nn
from PIL import Image
import requests
import numpy as np
import cv2

from models.depth_estimate import DepthEstimator
from models.rvln import RvlnMultiTask
from transformers import (
    InstructBlipProcessor,
    BertTokenizer,
    InstructBlipConfig,
    AutoTokenizer
)

def run_inference():
    # =================================================
    # 1. åŸºç¡€é…ç½®
    # =================================================
    MODEL_ID = "./instructblip-vicuna-7b"
    CHECKPOINT_PATH = "checkpoint/latest_checkpoint.pth"
    
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    # ITM æ¨ç†å»ºè®®ä½¿ç”¨ float16 æˆ– bfloat16
    DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
    
    print(f"æ­£åœ¨åˆå§‹åŒ– (Device: {DEVICE}, Main Dtype: {DTYPE})...")

    # =================================================
    # 2. é…ç½® Tokenizer å’Œ Config (å¿…é¡»æ­¥éª¤ï¼Œé˜²æ­¢æŠ¥é”™)
    # =================================================
    # å³ä½¿ ITM ä¸ç”¨ <history>ï¼Œæ¨¡å‹åˆå§‹åŒ–æ£€æŸ¥ä¹Ÿéœ€è¦å®ƒä»¬
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
    special_tokens = {"additional_special_tokens": ["<history>", "<current>"]}
    tokenizer.add_special_tokens(special_tokens)
    
    hist_id = tokenizer.convert_tokens_to_ids("<history>")
    curr_id = tokenizer.convert_tokens_to_ids("<current>")
    
    config = InstructBlipConfig.from_pretrained(MODEL_ID)
    config.history_token_id = hist_id
    config.current_token_id = curr_id

    # =================================================
    # 3. åŠ è½½æ¨¡å‹
    # =================================================
    print(">>> æ­£åœ¨åŠ è½½æ¨¡å‹...")
    processor = InstructBlipProcessor.from_pretrained(MODEL_ID)
    qformer_tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
    estimator = DepthEstimator(model_id="./Depth-Anything-V2-Small-hf", device=DEVICE)
    
    model = RvlnMultiTask.from_pretrained(
        MODEL_ID, 
        config=config,
        torch_dtype=DTYPE
    )
    model.language_model.resize_token_embeddings(len(tokenizer))
    
    # åŠ è½½å¾®è°ƒæƒé‡
    if os.path.exists(CHECKPOINT_PATH):
        print(f"ğŸ“¥ åŠ è½½æƒé‡: {CHECKPOINT_PATH}")
        checkpoint = torch.load(CHECKPOINT_PATH, map_location="cpu")
        if 'depth_backbone' in checkpoint:
            model.depth_backbone.load_state_dict(checkpoint['depth_backbone'], strict=True)
        if 'visual_fusion' in checkpoint:
            model.visual_fusion.load_state_dict(checkpoint['visual_fusion'], strict=True)
        if 'itm_head' in checkpoint:
            model.itm_head.load_state_dict(checkpoint['itm_head'], strict=True)
        if 'qformer' in checkpoint:
            model.qformer.load_state_dict(checkpoint['qformer'], strict=True)
        if 'query_tokens' in checkpoint:
            model.query_tokens.data = checkpoint['query_tokens'].data
    else:
        print("âš ï¸ æœªæ‰¾åˆ°æƒé‡ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–å‚æ•°ï¼")

    model.to(DEVICE)
    model.eval()
    
    # æ·±åº¦æ¨¡å‹é€šå¸¸éœ€è¦ float32 ä¿è¯ç²¾åº¦ï¼Œæˆ–è€…è·Ÿä¸»æ¨¡å‹ä¸€è‡´
    if hasattr(model, 'depth_model'):
        model.depth_model.to(dtype=torch.float32)

    # =================================================
    # 4. å‡†å¤‡å•å¼ æµ‹è¯•å›¾
    # =================================================
    print("\n>>> å‡†å¤‡æµ‹è¯•æ•°æ®...")
    img_path = "images/test2.jpg"
    if not os.path.exists(img_path):
        url = "http://images.cocodataset.org/val2017/000000039769.jpg"
        raw_image = Image.open(requests.get(url, stream=True).raw).convert("RGB")
        raw_image.save(img_path)
    else:
        raw_image = Image.open(img_path).convert("RGB")

    # 1. è·å– RGB Tensor [1, 3, H, W]
    inputs_rgb = processor(images=raw_image, return_tensors="pt")
    pixel_values = inputs_rgb.pixel_values.to(DEVICE, dtype=DTYPE)

    # 2. è·å– Depth Tensor [1, 3, H, W] (Processoré€šå¸¸è¾“å‡º3é€šé“)
    # ä½ çš„ forward_itm é‡Œæœ‰å…¼å®¹é€»è¾‘ï¼šif shape[1]==1: repeat
    depth_pil = estimator.predict_depth(raw_image, return_type="pil", colormap=cv2.COLORMAP_JET)
    inputs_depth = processor(images=depth_pil, return_tensors="pt")
    # æ³¨æ„ï¼šè¿™é‡Œä¿æŒ float32 ä¼ è¿›å»ï¼Œå› ä¸º forward_itm å†…éƒ¨ä¼šåš .to(dtype) è½¬æ¢
    depth_values = inputs_depth.pixel_values.to(DEVICE, dtype=torch.float32)

    # =================================================
    # 5. æ‰§è¡Œ ITM (ä¸€å¯¹å¤šåŒ¹é…)
    # =================================================
    print("\n" + "="*40)
    print("æµ‹è¯•: å•å›¾ vs å¤šæ–‡æœ¬åŒ¹é…")
    print("="*40)
    
    # å®šä¹‰å€™é€‰æ–‡æœ¬
    test_texts = [
        "A photo of two cats sleeping on a pink blanket.",  # è¿™é‡Œçš„æè¿°è¯·æ ¹æ®ä½ çš„æµ‹è¯•å›¾ä¿®æ”¹
        "A view of a modern kitchen with a refrigerator.",
        "Find the toilet."
    ]
    
    # ã€å…³é”®æ­¥éª¤ã€‘æ•°æ®å¯¹é½
    # ç°åœ¨çš„è¾“å…¥æ˜¯ 1 å¼ å›¾ï¼Œä½†æœ‰ N ä¸ªæ–‡æœ¬ã€‚
    # æˆ‘ä»¬éœ€è¦æŠŠ Image Tensor åœ¨ Batch ç»´åº¦å¤åˆ¶ N æ¬¡ï¼Œå˜æˆ [N, 3, H, W]
    # è¿™æ · forward_itm é‡Œçš„é€»è¾‘ num_images_per_sample å°±ä¼šç­‰äº 1
    
    batch_size = len(test_texts)
    
    # æ‰©å±• RGB: [1, 3, H, W] -> [B, 3, H, W]
    batch_pixel_values = pixel_values.repeat(batch_size, 1, 1, 1)
    
    # æ‰©å±• Depth: [1, 3, H, W] -> [B, 3, H, W]
    batch_depth_values = depth_values.repeat(batch_size, 1, 1, 1)
    
    # Tokenize æ–‡æœ¬: [B, Seq_Len]
    text_inputs = qformer_tokenizer(
        test_texts,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=32
    ).to(DEVICE)
    
    print("æ­£åœ¨è®¡ç®—åŒ¹é…åˆ†æ•°...")
    
    with torch.no_grad():
        # è°ƒç”¨ä½ æä¾›çš„ forward_itm
        # æ­¤æ—¶è¾“å…¥ç»´åº¦ï¼š
        # pixel_values:       [3, 3, 224, 224]
        # depth_pixel_values: [3, 3, 224, 224]
        # input_ids:          [3, 32]
        logits = model.forward_itm(
            pixel_values=batch_pixel_values,
            depth_pixel_values=batch_depth_values,
            input_ids=text_inputs.input_ids,
            attention_mask=text_inputs.attention_mask
        )
        
        # Softmax è·å–æ¦‚ç‡ (å‡è®¾ class 1 æ˜¯åŒ¹é…ï¼Œclass 0 æ˜¯ä¸åŒ¹é…)
        # ä½ çš„ itm_head è¾“å‡ºç»´åº¦æ˜¯ [B, 2]
        probs = torch.softmax(logits, dim=1)
        
    print("\n>>> åŒ¹é…ç»“æœ:")
    for i, text in enumerate(test_texts):
        # index 1 é€šå¸¸ä»£è¡¨ "Match" (å–å†³äºä½ çš„è®­ç»ƒ Label è®¾ç½®ï¼Œé€šå¸¸ 1=Pos, 0=Neg)
        score = probs[i][1].item() 
        
        # å¯è§†åŒ–è¿›åº¦æ¡
        bar_len = int(score * 20)
        bar = "â–ˆ" * bar_len + "â–‘" * (20 - bar_len)
        
        print(f"Text: {text:<45} | Score: {score:.4f} | {bar}")

if __name__ == "__main__":
    run_inference()
