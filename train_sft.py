import os
import torch
import torch.nn as nn
import torch.distributed as dist
from torch.utils.data import random_split
from transformers import (
    InstructBlipProcessor,
    InstructBlipConfig,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig
)
from peft import (
    LoraConfig,
    get_peft_model,
    TaskType
)

# ==========================================
# [SwanLab] 1. å¼•å…¥ SwanLab å’Œ HF å›è°ƒ
# ==========================================
import swanlab
from swanlab.integration.huggingface import SwanLabCallback

# å¼•å…¥ä½ çš„è‡ªå®šä¹‰æ¨¡å—
from models.rvln import InstructBlipMultiTask 
# å¼•å…¥ä½ ä¸Šé¢æä¾›çš„ Dataset å’Œ Collator ç±»
from data_utils import InstructBlipLoRADataset, DataCollatorForInstructBlip 

def print_trainable_parameters(model):
    """æ‰“å°å¯è®­ç»ƒå‚æ•°ç»Ÿè®¡"""
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"trainable params: {trainable_params} || all params: {all_param} || "
        f"trainable%: {100 * trainable_params / all_param:.2f}"
    )


# ==========================================
# 2. ä¿®æ­£ Data Collator ä»¥åŒ¹é…æ¨¡å‹è¾“å…¥
# ==========================================
class DataCollatorWrapper(DataCollatorForInstructBlip):
    """
    åŒ…è£…ä½ åŸæœ¬çš„ Collatorï¼Œå°†è¾“å‡ºçš„é”®åä¿®æ”¹ä¸ºæ¨¡å‹ forward å‡½æ•°éœ€è¦çš„åå­—
    pixel_values_rgb -> pixel_values
    pixel_values_depth -> depth_pixel_values
    """
    def __call__(self, batch):
        outputs = super().__call__(batch)
        
        # é‡å‘½åé”®å€¼ä»¥åŒ¹é… InstructBlipMultiTask.forward çš„å‚æ•°
        if "pixel_values_rgb" in outputs:
            outputs["pixel_values"] = outputs.pop("pixel_values_rgb")
        
        if "pixel_values_depth" in outputs:
            outputs["depth_pixel_values"] = outputs.pop("pixel_values_depth")
            
        return outputs

# ==========================================
# 3. è‡ªå®šä¹‰ Trainer (ç¡®ä¿ä¿å­˜ Embeddings)
# ==========================================
class CustomTrainer(Trainer):
    def save_model(self, output_dir=None, _internal_call=False):
        """é‡å†™ä¿å­˜é€»è¾‘ï¼Œç¡®ä¿ LoRA + Embeddings + Tokenizer éƒ½èƒ½è¢«ä¿å­˜"""
        if output_dir is None:
            output_dir = self.args.output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        # 1. ä¿å­˜ LoRA å’Œ modules_to_save (embed_tokens)
        super().save_model(output_dir, _internal_call)
        
        # 2. ä¿å­˜ Tokenizer
        if self.is_world_process_zero():
            self.tokenizer.save_pretrained(output_dir)
            
            print(f"âœ… Model (LoRA + Embeddings) saved to {output_dir}")

class WeightedTrainer(CustomTrainer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # è·å–æ•°å­— -1, 0-8 çš„ Token ID
        # æ³¨æ„ï¼šä¸åŒ Tokenizer å¯¹æ•°å­—çš„å¤„ç†ä¸åŒï¼Œæœ‰å¯èƒ½æ˜¯ "8" ä¹Ÿæœ‰å¯èƒ½æ˜¯ " 8" (å¸¦ç©ºæ ¼)
        # è¿™é‡ŒæŠŠå¸¸è§å¯èƒ½éƒ½åŠ è¿›å»ï¼Œç¡®ä¿ä¸‡æ— ä¸€å¤±
        self.target_tokens = set()
        for i in range(-1, 9): # -1 åˆ° 8
            # çº¯æ•°å­—
            self.target_tokens.add(self.tokenizer.convert_tokens_to_ids(str(i)))
            # å¸¦ç©ºæ ¼çš„æ•°å­— (SentencePiece å¸¸è§)
            self.target_tokens.add(self.tokenizer.convert_tokens_to_ids(" " + str(i)))
        
        # å¤„ç† "-1" è¿™ç§æƒ…å†µï¼ŒTokenzier å¯èƒ½ä¼šæŠŠå®ƒæ‹†æˆ "-" å’Œ "1"
        # å¦‚æœä½ æƒ³æŠŠ "-" ä¹ŸåŠ æƒï¼Œå¯ä»¥åŠ ä¸Š
        self.target_tokens.add(self.tokenizer.convert_tokens_to_ids("-"))

        # æƒé‡å€æ•°ï¼šå…³é”® Token çš„ Loss æ”¾å¤§ 10 å€
        self.key_token_weight = 10.0

    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        """
        é‡å†™ Loss è®¡ç®—é€»è¾‘ï¼Œå¯¹æ•°å­— Token è¿›è¡ŒåŠ æƒ
        """
        # 1. æ­£å¸¸çš„å‰å‘ä¼ æ’­
        labels = inputs.get("labels")
        outputs = model(**inputs)
        
        # 2. è·å– Logits
        logits = outputs.get("logits")
        
        # 3. ç§»ä½ (Shift) ä»¥é€‚é… Causal LM
        # é¢„æµ‹ç¬¬ i ä¸ª token ç”¨çš„æ˜¯ç¬¬ i-1 ä¸ª token çš„è¾“å‡º
        shift_logits = logits[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()
        
        # 4. å±•å¹³
        batch_size, seq_len, vocab_size = shift_logits.shape
        flat_logits = shift_logits.view(-1, vocab_size)
        flat_labels = shift_labels.view(-1)
        
        # 5. è®¡ç®—æœªç¼©å‡çš„ CrossEntropy Loss (reduction='none')
        # è¿™æ ·æˆ‘ä»¬ä¼šå¾—åˆ°æ¯ä¸€ä¸ª Token çš„ Lossï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå¹³å‡å€¼
        loss_fct = nn.CrossEntropyLoss(reduction='none')
        # åªéœ€è¦è®¡ç®— label != -100 çš„éƒ¨åˆ†
        token_losses = loss_fct(flat_logits, flat_labels)
        
        # 6. åˆ›å»ºæƒé‡ Mask
        # é»˜è®¤æƒé‡ä¸º 1.0
        weights = torch.ones_like(token_losses)
        
        # æ‰¾åˆ° Label æ˜¯æ•°å­—çš„åœ°æ–¹ï¼Œå°†æƒé‡è®¾ä¸º 10.0
        # è¿™æ˜¯ä¸€ä¸ª Tensor æ“ä½œï¼Œé€Ÿåº¦å¾ˆå¿«
        for target_id in self.target_tokens:
            weights[flat_labels == target_id] = self.key_token_weight
            
        # 7. åº”ç”¨æƒé‡
        weighted_loss = token_losses * weights
        
        # 8. å–å¹³å‡ (åªå¯¹é Mask çš„éƒ¨åˆ†å–å¹³å‡)
        # ç»Ÿè®¡æœ‰æ•ˆ Token æ•°é‡ (labels != -100)
        active_elements = (flat_labels != -100).sum()
        
        if active_elements > 0:
            final_loss = weighted_loss.sum() / active_elements
        else:
            final_loss = weighted_loss.sum()

        return (final_loss, outputs) if return_outputs else final_loss
def main():
    # =================Configuration=================
    model_name_or_path = "./instructblip-vicuna-7b" 
    # ä¹‹å‰è®­ç»ƒå¥½çš„ Stage 1 æƒé‡è·¯å¾„ (åŒ…å« Fusion, Q-Former, Depth ç­‰)
    stage1_checkpoint = "checkpoint/latest_checkpoint.pth"
    
    data_path = "dataset_waypoint/rgb_images_r2r_train_processed.json"
    output_dir = "./output/rvln_sft_llm"
    
    # è®­ç»ƒå‚æ•°
    batch_size = 2
    grad_accumulation = 4 # ç¨å¾®åŠ å¤§ç´¯ç§¯ï¼Œæ¨¡æ‹Ÿæ›´å¤§ batch
    learning_rate = 5e-5  # SFT LLM å­¦ä¹ ç‡
    num_epochs = 3
    lora_rank = 32
    lora_alpha = 64
    
    # ================= [SwanLab] 2. åˆå§‹åŒ– SwanLab =================
    # åœ¨è¿™é‡Œå®šä¹‰å®éªŒåç§°å’Œéœ€è¦è®°å½•çš„é…ç½®ä¿¡æ¯
    swanlab.init(
        project="InstructBlip-LoRA-SFT",
        experiment_name="vicuna-7b-lora-stage2",
        description="InstructBlip Stage 2 SFT with LoRA monitoring",
        config={
            "model_name": model_name_or_path,
            "stage1_checkpoint": stage1_checkpoint,
            "data_path": data_path,
            "batch_size": batch_size,
            "grad_accumulation": grad_accumulation,
            "learning_rate": learning_rate,
            "num_epochs": num_epochs,
            "lora_rank": lora_rank,
            "lora_alpha": lora_alpha,
            "lora_dropout": 0.05,
            "modules_to_save": ["embed_tokens", "lm_head"]
        }
    )
    
    # =================1. Processor & Tokenizer=================
    print("Loading Processor...")
    processor = InstructBlipProcessor.from_pretrained(model_name_or_path)
    tokenizer = processor.tokenizer
    qformer_tokenizer = processor.qformer_tokenizer

    # æ·»åŠ ç‰¹æ®Š Token
    special_tokens_dict = {'additional_special_tokens': ["<history>", "<current>"]}
    tokenizer.add_special_tokens(special_tokens_dict)
    
    history_token_id = tokenizer.convert_tokens_to_ids("<history>")
    current_token_id = tokenizer.convert_tokens_to_ids("<current>")

    # =================2. Model Initialization=================
    print("Loading Base Model...")
    config = InstructBlipConfig.from_pretrained(model_name_or_path)
    config.history_token_id = history_token_id
    config.current_token_id = current_token_id

    # åŠ è½½åŸºç¡€æ¨¡å‹
    model = InstructBlipMultiTask.from_pretrained(
        model_name_or_path,
        config=config,
        torch_dtype=torch.float16
    )

    # è°ƒæ•´ Embedding å¤§å°
    model.language_model.resize_token_embeddings(len(tokenizer))

    # =================3. [å…³é”®] åŠ è½½ Stage 1 è®­ç»ƒå¥½çš„æƒé‡=================
    if os.path.exists(stage1_checkpoint):
        print(f"ğŸ“¥ Loading Stage 1 Checkpoint from: {stage1_checkpoint}")
        ckpt = torch.load(stage1_checkpoint, map_location="cpu")
        
        msg = model.load_state_dict(ckpt, strict=False) 
        print(f"Checkpoint Load Status: {msg}")
        
        if 'visual_fusion' in ckpt: print(" - Visual Fusion Loaded âœ…")
        if 'qformer' in ckpt: print(" - Q-Former Loaded âœ…")
        if 'depth_backbone' in ckpt: print(" - Depth Backbone Loaded âœ…")
    else:
        print("âŒ Warning: Stage 1 checkpoint not found! Training from scratch (Not Recommended).")

    # =================4. Freeze & LoRA Setup=================
    
    # 4.1 å…¨å±€å†»ç»“
    for param in model.parameters():
        param.requires_grad = False
        
    # 4.2 é…ç½® LoRA (é’ˆå¯¹ LLM)
    peft_config = LoraConfig(
        r=lora_rank,
        lora_alpha=lora_alpha,
        # é’ˆå¯¹ Vicuna/Llama çš„æ‰€æœ‰çº¿æ€§å±‚
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type=TaskType.CAUSAL_LM,
        # âš ï¸ å…³é”®ï¼šå› ä¸ºåŠ äº†æ–° tokenï¼Œå¿…é¡»è®­ç»ƒ Embedding å±‚å’Œ Head
        modules_to_save=["embed_tokens", "lm_head"] 
    )
    
    print("Applying LoRA to LLM...")
    model.language_model = get_peft_model(model.language_model, peft_config)
    
    print_trainable_parameters(model)

# ================= 5. Data Setup (å…³é”®ä¿®æ”¹ï¼šåˆ’åˆ†éªŒè¯é›†) =================
    print("Loading Full Dataset...")
    full_dataset = InstructBlipLoRADataset(
        data_path=data_path,
        processor=processor,
        tokenizer=tokenizer,
        image_root="", 
        history_len=4,
        current_len=1
    )
    
    # [æ–°å¢] è®¡ç®—åˆ’åˆ†æ•°é‡
    val_ratio = 0.01  # 1% åšéªŒè¯ï¼Œ99% è®­ç»ƒ
    val_size = int(len(full_dataset) * val_ratio)
    train_size = len(full_dataset) - val_size
    
    print(f"Splitting Dataset: Total={len(full_dataset)} | Train={train_size} | Val={val_size}")
    
    # [æ–°å¢] éšæœºåˆ‡åˆ†
    # generatorç”¨äºå›ºå®šéšæœºç§å­ï¼Œä¿è¯æ¯æ¬¡åˆ‡åˆ†ä¸€æ ·ï¼Œæ–¹ä¾¿å¤ç°
    train_dataset, eval_dataset = random_split(
        full_dataset, 
        [train_size, val_size],
        generator=torch.Generator().manual_seed(42) 
    )
    
    collator = DataCollatorWrapper(
        processor=processor,
        tokenizer=tokenizer,
        qformer_tokenizer=qformer_tokenizer
    )

    # ================= 6. Trainer Setup (å…³é”®ä¿®æ”¹ï¼šæ·»åŠ  Eval é…ç½®) =================
    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=batch_size,
        gradient_accumulation_steps=grad_accumulation,
        learning_rate=learning_rate,
        num_train_epochs=num_epochs,
        fp16=True,
        deepspeed="./ds_config_zero2.json",
        remove_unused_columns=False,
        report_to="none",
        
        # --- [æ–°å¢] éªŒè¯é›†ç›¸å…³é…ç½® ---
        evaluation_strategy="steps",   # æŒ‰æ­¥æ•°è¯„ä¼° (ä¹Ÿå¯ä»¥é€‰ "epoch")
        eval_steps=1000,                # æ¯ 100 æ­¥è¯„ä¼°ä¸€æ¬¡éªŒè¯é›† (æ ¹æ®ä½ æ€»æ­¥æ•°è°ƒæ•´)
        per_device_eval_batch_size=batch_size, # éªŒè¯é›†çš„ Batch Size
        
        # --- [æ–°å¢] æ¨¡å‹ä¿å­˜ç­–ç•¥ (Save Best) ---
        save_strategy="steps",         # å¿…é¡»å’Œ evaluation_strategy ä¸€è‡´
        save_steps=2000,                # æ¯ 2000 æ­¥å°è¯•ä¿å­˜
        save_total_limit=2,            # æœ€å¤šä¿ç•™ 2 ä¸ª checkpointï¼Œçœç¡¬ç›˜
        load_best_model_at_end=True,   # è®­ç»ƒç»“æŸæ—¶ï¼Œè‡ªåŠ¨åŠ è½½éªŒè¯é›†æ•ˆæœæœ€å¥½çš„æ¨¡å‹
        metric_for_best_model="loss",  # ä»¥ loss ä¸ºæ ‡å‡† (loss è¶Šå°è¶Šå¥½)
        greater_is_better=False,       # loss æ˜¯è¶Šå°è¶Šå¥½ï¼Œæ‰€ä»¥æ˜¯ False
        logging_steps=5,
    )

    # ä½¿ç”¨è‡ªå®šä¹‰ Trainer
    trainer = WeightedTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=collator,
        processing_class=tokenizer,
        # ================= [SwanLab] 3. æ·»åŠ  Callback =================
        # SwanLabCallback ä¼šè‡ªåŠ¨è®°å½• Loss, LR, Epoch ç­‰ä¿¡æ¯
        callbacks=[SwanLabCallback()]
    )

    trainer.train()
    trainer.accelerator.wait_for_everyone()
    
    # ä»…ç”±ä¸»è¿›ç¨‹è§¦å‘ä¿å­˜é€»è¾‘ï¼ˆæˆ–è€… trainer.save_model å†…éƒ¨ä¼šå¤„ç†ï¼Œä½†åŠ ä¸Š wait æ›´å®‰å…¨ï¼‰
    if trainer.is_world_process_zero():
        trainer.save_model(output_dir)
    if dist.is_initialized():
        dist.destroy_process_group()

if __name__ == "__main__":
    main()