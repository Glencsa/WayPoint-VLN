import json
import numpy as np
from habitat import Env
from habitat.core.agent import Agent
from tqdm import trange
import os
import re
import torch
import cv2
import imageio
from habitat.utils.visualizations import maps
import random
import sys
from PIL import Image
import cv2
from point_project import process_depth_rgb_simple,process_depth_rgb_highlight

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
current_path = os.path.abspath(__file__)
project_root = os.path.dirname(os.path.dirname(os.path.dirname(current_path)))
sys.path.insert(0, project_root)

from transformers import InstructBlipProcessor
from models.rvln import RvlnMultiTask
from utils.utils import prepare_inputs_for_generate


class RVLN_Agent(Agent):
    def __init__(self, model_path, result_path, exp_save):
        
        print("Initialize RVLN Agent")
        
        self.result_path = result_path
        self.require_map = True if "video" in exp_save else False
        self.require_data = True if "data" in exp_save else False
        self.model_path = model_path
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„ï¼ˆä¸ NaVid å®Œå…¨ä¸€è‡´ï¼‰
        if self.require_map or self.require_data:
            os.makedirs(self.result_path, exist_ok=True)
        
        if self.require_data:
            os.makedirs(os.path.join(self.result_path, "log"), exist_ok=True)
        
        if self.require_map:
            os.makedirs(os.path.join(self.result_path, "video"), exist_ok=True)
        
        # è®¾ç½®è®¾å¤‡
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.dtype = torch.float16
        
        # åŠ è½½æ¨¡å‹
        print(f"Loading RVLN model from: {self.model_path}")
        try:
            self.processor = InstructBlipProcessor.from_pretrained(self.model_path)
            self.tokenizer = self.processor.tokenizer
            self.tokenizer.padding_side = "right"
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                
            self.model = RvlnMultiTask.from_pretrained(
                self.model_path,
                torch_dtype=self.dtype,
            ).to(self.device)
            stage1_checkpoint = "/home/isvl/guan_code/WayPoint-VLNoutput/stage1_checkpoint/latest_checkpoint.pth"
            if os.path.exists(stage1_checkpoint):
                print(f"ğŸ“¥ Loading Stage 1 Checkpoint from: {stage1_checkpoint}")
                ckpt = torch.load(stage1_checkpoint, map_location="cpu")

                msg = self.model.load_state_dict(ckpt, strict=False)
                print(f"Checkpoint Load Status: {msg}")
                
                if 'visual_fusion' in ckpt: print(" - Visual Fusion Loaded âœ…")
                if 'qformer' in ckpt: print(" - Q-Former Loaded âœ…")
                if 'depth_backbone' in ckpt: print(" - Depth Backbone Loaded âœ…")
            else:
                print("âŒ Warning: Stage 1 checkpoint not found! Training from scratch (Not Recommended).")

            self.model.eval()
            
            print("RVLN Agent Initialization Complete")
            
        except Exception as e:
            print(f"[ERROR] Failed to load RVLN model: {e}")
            print("Please check:")
            print("  1. Model path exists and contains all required files")
            print("  2. config.json is valid JSON format")
            print("  3. Vision encoder config is properly set")
            raise
        

        # å†å²è§‚æµ‹é˜Ÿåˆ—ï¼ˆä¿å­˜ numpy arraysï¼‰
        self.history_rgb_list = []
        self.history_depth_list = []
        self.max_history = 5  # æœ€å¤šä¿å­˜5å¸§å†å²
        
        # å¯è§†åŒ–ç›¸å…³ï¼ˆä¸ NaVid å®Œå…¨ä¸€è‡´ï¼‰
        self.rgb_list = []
        self.depth_list = []
        self.topdown_map_list = []
        
        # çŠ¶æ€ç®¡ç†
        self.count_id = 0
        self.pending_action_list = []
        self.episode_id = None
        
        self.reset()
    
    def reset(self):
        """é‡ç½® agent çŠ¶æ€ï¼ˆä¸ NaVid å®Œå…¨ä¸€è‡´çš„é€»è¾‘ï¼‰"""
        
        # ä¿å­˜ä¸Šä¸€ä¸ª episode çš„è§†é¢‘
        if self.require_map:
            if len(self.topdown_map_list) != 0:
                output_video_path = os.path.join(self.result_path, "video", "{}.gif".format(self.episode_id))
                imageio.mimsave(output_video_path, self.topdown_map_list)
        
        # é‡ç½®æ‰€æœ‰çŠ¶æ€
        self.history_rgb_list = []
        self.history_depth_list = []
        self.rgb_list = []
        self.depth_list = []
        self.topdown_map_list = []
        self.pending_action_list = []
        self.count_id += 1
    
    def process_observations(self, rgb, depth):
        """
        å¤„ç†å¹¶æ·»åŠ æ–°çš„è§‚æµ‹åˆ°å†å²é˜Ÿåˆ—
        
        Args:
            rgb: RGB observation (numpy array)
            depth: Depth observation (numpy array)
        """
        # # RGB å¤„ç†ï¼šç¡®ä¿æ˜¯ uint8 æ ¼å¼
        # if rgb.dtype != np.uint8:
        #     if rgb.max() <= 1.0:
        #         rgb = (rgb * 255.0).clip(0, 255).astype(np.uint8)
        #     else:
        #         rgb = rgb.clip(0, 255).astype(np.uint8)
        
        # # Depth å¤„ç†ï¼šå½’ä¸€åŒ–åˆ° 0-255ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
        # depth_vis = depth.copy()
        # if depth_vis.dtype != np.uint8:
        #     depth_float = depth_vis.astype(np.float32)
        #     depth_float = np.nan_to_num(depth_float, nan=0.0, posinf=0.0, neginf=0.0)
            
        #     if depth_float.ndim == 3:
        #         if depth_float.shape[2] == 1:
        #             depth_float = depth_float[..., 0]
        #         else:
        #             depth_float = depth_float.mean(axis=2)
            
        #     d_min, d_max = float(depth_float.min()), float(depth_float.max())
        #     if d_max > d_min:
        #         depth_norm = (depth_float - d_min) / (d_max - d_min)
        #     else:
        #         depth_norm = np.zeros_like(depth_float)
            
        #     depth_vis = (depth_norm * 255.0).clip(0, 255).astype(np.uint8)
        
        # æ·»åŠ åˆ°å†å²é˜Ÿåˆ—
        self.history_rgb_list.append(rgb)
        self.history_depth_list.append(depth)  # ä¿å­˜åŸå§‹ depthï¼Œæ¨¡å‹å†…éƒ¨ä¼šå¤„ç†
        
        # ä¿å­˜ç”¨äºå¯è§†åŒ–
        self.rgb_list.append(rgb)
        self.depth_list.append(depth)
        
        # ä¿æŒå†å²é•¿åº¦é™åˆ¶
        if len(self.history_rgb_list) > self.max_history:
            self.history_rgb_list = self.history_rgb_list[-self.max_history:]
            self.history_depth_list = self.history_depth_list[-self.max_history:]
    
    def predict_route(self, instruction, max_new_tokens=100):
        """
        åŸºäºå½“å‰å†å²è§‚æµ‹é¢„æµ‹è·¯çº¿
        
        Args:
            instruction: å¯¼èˆªæŒ‡ä»¤æ–‡æœ¬
            max_new_tokens: ç”Ÿæˆçš„æœ€å¤§ token æ•°
            
        Returns:
            route_number: é¢„æµ‹çš„è·¯çº¿ç¼–å· (-1 to 8)
            output_text: æ¨¡å‹è¾“å‡ºçš„å®Œæ•´æ–‡æœ¬
        """
        if len(self.history_rgb_list) == 0:
            raise RuntimeError("[RVLN Agent] No observations available.")
        

        # for idx, rgb_img in enumerate(self.history_rgb_list):
        #     cv2.imwrite(f"output_rgb_vis_{idx}.png", rgb_img)
        # for idx, depth_img in enumerate(self.history_depth_list):
        #     cv2.imwrite(f"output_depth_vis_{idx}.png", depth_img)
        # å‡†å¤‡è¾“å…¥ï¼ˆè‡ªåŠ¨è¡¥é½åˆ°5å¸§ï¼‰
        inputs = prepare_inputs_for_generate(
            self.history_rgb_list,
            self.history_depth_list,
            instruction,
            self.processor,
            self.device
        )
        
        # ç”Ÿæˆé¢„æµ‹
        with torch.no_grad():
            outputs = self.model.generate(
                pixel_values=inputs["pixel_values"],
                depth_pixel_values=inputs["depth_pixel_values"],
                qformer_input_ids=inputs["qformer_input_ids"],
                qformer_attention_mask=inputs["qformer_attention_mask"],
                input_ids=inputs["input_ids"],
                attention_mask=inputs["attention_mask"],
                max_new_tokens=max_new_tokens,
                do_sample=False,
                repetition_penalty=1.0
            )
        
        # è§£ç è¾“å‡º
        print(f"[RVLN Agent] Model raw output IDs: {outputs}")
        output_text = self.processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()
        print(f"[RVLN Agent] Model output: {output_text}")
        # è§£æè·¯çº¿ç¼–å·
        route_number = self._parse_route_number(output_text)
        
        return route_number, output_text
    
    def _parse_route_number(self, output_text):
        """
        ä»æ¨¡å‹è¾“å‡ºæ–‡æœ¬ä¸­è§£æè·¯çº¿ç¼–å·
        
        æ”¯æŒçš„æ ¼å¼:
        - {'Route': 3}
        - "Route": 3
        - Route: 3
        - çº¯æ•°å­—: 3
        """
        # å°è¯•åŒ¹é… Route: N æ ¼å¼
        match = re.search(r"['\"]?Route['\"]?\s*[:=]\s*(-?\d+)", output_text, re.IGNORECASE)
        if match:
            return int(match.group(1))
        
        # å°è¯•åŒ¹é…çº¯æ•°å­—
        match = re.search(r"(-?\d+)", output_text)
        if match:
            return int(match.group(1))
        
        # é»˜è®¤è¿”å› -1 (åœæ­¢)
        return -1
    
    def route_to_actions(self, route_number):
        """
        å°†è·¯çº¿ç¼–å·è½¬æ¢ä¸ºåŠ¨ä½œåºåˆ—
        
        è·¯çº¿ç¼–å·æ˜ å°„:
        0: stop
        1: å·¦, å·¦, å·¦, å‰
        2: å·¦, å·¦, å‰
        3: å·¦, å‰
        4: å‰
        5: å³, å‰
        6: å³, å³, å‰
        7: å³, å³, å³, å‰
        8: å³, å³, å³, å³, å‰
        
        Returns:
            action_list: åŠ¨ä½œåºåˆ— [action_id, ...]
        """
        action_list = []
        if route_number == -1:
            action_list.extend([0])  # stop
        elif route_number == 0:
            action_list.extend([2, 2, 2, 2])  # å·¦, å·¦, å·¦, å·¦
        elif route_number == 1:
            action_list.extend([2, 2, 1])  # å·¦, å·¦, å·¦, å‰
        elif route_number == 2:
            action_list.extend([2, 1])  # å·¦, å·¦, å‰
        elif route_number == 3:
            action_list.extend([2, 1, 1, 1])  # å·¦, å‰
        elif route_number == 4:
            action_list.extend([1, 1, 1, 1])  # å‰
        elif route_number == 5:
            action_list.extend([3, 1, 1, 1])  # å³, å‰
        elif route_number == 6:
            action_list.extend([3, 3, 1, 1])  # å³, å³, å‰
        elif route_number == 7:
            action_list.extend([3, 3, 3, 1])  # å³, å³, å³, å‰
        elif route_number == 8:
            action_list.extend([3, 3, 3, 3])  # å³, å³, å³, å³, å‰
        else:
            action_list.extend([random.randint(1, 3) for _ in range(random.randint(1, 3))])  # é»˜è®¤1-3éšæœºæ•°
        return action_list
    
    def addtext(self, image, instruction, navigation):
        """åœ¨å›¾åƒä¸Šæ·»åŠ æ–‡æœ¬ï¼ˆä¸ NaVid å®Œå…¨ä¸€è‡´ï¼‰"""
        h, w = image.shape[:2]
        new_height = h + 150
        new_image = np.zeros((new_height, w, 3), np.uint8)
        new_image.fill(255)  
        new_image[:h, :w] = image

        font = cv2.FONT_HERSHEY_SIMPLEX
        textsize = cv2.getTextSize(instruction, font, 0.5, 2)[0]
        textY = h + (50 + textsize[1]) // 2

        y_line = textY + 0 * textsize[1]

        words = instruction.split(' ')
        x = 10
        line = ""

        for word in words:
            test_line = line + ' ' + word if line else word
            test_line_size, _ = cv2.getTextSize(test_line, font, 0.5, 2)

            if test_line_size[0] > image.shape[1] - x:
                cv2.putText(new_image, line, (x, y_line), font, 0.5, (0, 0, 0), 2)
                line = word
                y_line += textsize[1] + 5
            else:
                line = test_line

        if line:
            cv2.putText(new_image, line, (x, y_line), font, 0.5, (0, 0, 0), 2)

        y_line = y_line + 1 * textsize[1] + 10
        new_image = cv2.putText(new_image, navigation, (x, y_line), font, 0.5, (0, 0, 0), 2)

        return new_image
    
    def act(self, observations, info, episode_id):
        """
        ä¸»è¦çš„åŠ¨ä½œé€‰æ‹©æ¥å£ï¼ˆä¸ NaVid å®Œå…¨ä¸€è‡´çš„æ¥å£ï¼‰
        
        Args:
            observations: Habitat è§‚æµ‹å­—å…¸ï¼ŒåŒ…å« rgb, depth, instruction ç­‰
            info: ç¯å¢ƒä¿¡æ¯å­—å…¸ï¼ŒåŒ…å« top_down_map_vlnce ç­‰
            episode_id: å½“å‰ episode çš„ ID
            
        Returns:
            {"action": action_id}
        """
        self.episode_id = episode_id
        
        # æå– RGB å’Œ Depth
        rgb = observations["rgb"]
        depth = observations["depth"]
        if depth is None:
            raise ValueError("[RVLN Agent] Missing depth in observations")
        
        # æ·»åŠ æŠ•å½±é€»è¾‘
        # rgb=cv2.imread(rgb)
        # depth=cv2.imread(depth, cv2.IMREAD_ANYDEPTH)
        rgb_point,points_3d,point_ids=process_depth_rgb_simple(depth,rgb)
        
        depth=normalize_depth_for_vis(depth)
        cv2.imwrite("debug_rgb.png",rgb_point)
        cv2.imwrite("debug_depth.png",depth)

       
        # å¤„ç†è§‚æµ‹
        self.process_observations(rgb_point, depth)
        
        
        # å¦‚æœæœ‰å¾…æ‰§è¡Œçš„åŠ¨ä½œï¼Œä¼˜å…ˆæ‰§è¡Œï¼ˆä¸ NaVid å®Œå…¨ä¸€è‡´ï¼‰
        if len(self.pending_action_list) != 0:
            temp_action = self.pending_action_list.pop(0)
            
            return {"action": temp_action}
        
        # ä½¿ç”¨ RVLN æ¨¡å‹é¢„æµ‹è·¯çº¿
        instruction = observations["instruction"]["text"]
        print(f"[RVLN Agent] Instruction: {instruction}")

        route_number, output_text = self.predict_route(instruction)
        print(f"[RVLN Agent] Predicted route number: {route_number}")
        # ç”Ÿæˆå¯è§†åŒ–æ–‡æœ¬
        navigation_text = f"Route: {route_number} | {output_text}"
        # ç”Ÿæˆå¯è§†åŒ–å›¾åƒï¼ˆä¸ NaVid ä¸€è‡´ï¼‰
        if self.require_map:
            top_down_map = maps.colorize_draw_agent_and_fit_to_height(
                info["top_down_map_vlnce"], rgb.shape[0]
            )
            # æ‹¼æ¥ RGB å’Œ top-down map
            rgb =process_depth_rgb_highlight(depth,rgb,selected_ids=[route_number])
            output_im = np.concatenate((rgb, top_down_map), axis=1)
        if self.require_map:
            img = self.addtext(output_im, observations["instruction"]["text"],navigation_text)
            self.topdown_map_list.append(img)

        
        if self.require_map:
            img = self.addtext(output_im, instruction, navigation_text)
            self.topdown_map_list.append(img)
        
        # å°†è·¯çº¿è½¬æ¢ä¸ºåŠ¨ä½œåºåˆ—
        action_list = self.route_to_actions(route_number)
        
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆåŠ¨ä½œï¼Œä½¿ç”¨éšæœºåŠ¨ä½œ
        if len(action_list) == 0:
            action_list.append(random.randint(1, 3))
        
        # å°†åŠ¨ä½œåŠ å…¥å¾…æ‰§è¡Œé˜Ÿåˆ—
        self.pending_action_list.extend(action_list)
        
        # è¿”å›ç¬¬ä¸€ä¸ªåŠ¨ä½œ
        return {"action": self.pending_action_list.pop(0)}
    
def normalize_depth_for_vis(depth):
    """
    å°†åŸå§‹æ·±åº¦å›¾å½’ä¸€åŒ–åˆ° 0-255ï¼Œé€‚åˆå¯è§†åŒ–
    Args:
        depth: åŸå§‹æ·±åº¦å›¾ (numpy array)
    Returns:
        depth_vis: å½’ä¸€åŒ–åçš„ uint8 æ·±åº¦å›¾
    """
    import numpy as np
    depth_float = depth.astype(np.float32)
    depth_float = np.nan_to_num(depth_float, nan=0.0, posinf=0.0, neginf=0.0)
    d_min, d_max = float(depth_float.min()), float(depth_float.max())
    if d_max > d_min:
        depth_norm = (depth_float - d_min) / (d_max - d_min)
    else:
        depth_norm = np.zeros_like(depth_float)
    depth_vis = (depth_norm * 255.0).clip(0, 255).astype(np.uint8)
    return depth_vis