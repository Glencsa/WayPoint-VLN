import os
import random
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from PIL import Image
from transformers import InstructBlipProcessor, BertTokenizer
from InstructBlip import InstructBlipMultiTask
import swanlab
# ==============================================================================
# 2. Dataset å®šä¹‰ (ä¿®æ”¹ä¸ºè¿”å› PIL)
# ==============================================================================
class Flickr30kDataset(Dataset):
    def __init__(self, image_root, caption_file):
        self.image_root = image_root
        self.samples = []
        
        print("æ­£åœ¨åŠ è½½æ•°æ®é›†ç´¢å¼•...")
        self.image_to_captions = {}
        with open(caption_file, "r", encoding="utf-8") as f:
            for line in f:
                parts = line.strip().split("\t", 1)
                if len(parts) < 2: continue # è·³è¿‡åè¡Œ
                name_and_id, caption = parts
                image_name = name_and_id.split("#")[0]
                
                if image_name not in self.image_to_captions:
                    self.image_to_captions[image_name] = []
                self.image_to_captions[image_name].append(caption)
        
        for image_name, captions in self.image_to_captions.items():
            for caption in captions:
                self.samples.append((image_name, caption))
        
        self.image_names = list(self.image_to_captions.keys())
        print(f"æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…± {len(self.samples)} ä¸ªæ ·æœ¬ã€‚")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        image_name, caption = self.samples[idx]
        image_path = os.path.join(self.image_root, image_name)
        
        # --- æ”¹åŠ¨ç‚¹ï¼šç›´æ¥è¿”å› PIL Image ---
        # InstructBlipProcessor ä¼šå¤„ç† Resize å’Œ Normalizeï¼Œä¸è¦åœ¨è¿™é‡Œè½¬ Tensor
        try:
            img = Image.open(image_path).convert("RGB")
        except Exception as e:
            print(f"Error loading image {image_path}: {e}")
            # è¿”å›ä¸€ä¸ªçº¯é»‘å›¾ç‰‡é˜²æ­¢å´©æºƒ
            img = Image.new('RGB', (224, 224), color='black')
            
        return img, caption, image_name

def collate_fn(batch):
    # ç®€å•çš„ list æ‰“åŒ…ï¼Œä¸åš tensor è½¬æ¢ï¼Œå› ä¸ºåé¢è¦åšè´Ÿé‡‡æ ·
    images, captions, image_names = zip(*batch)
    return list(images), list(captions), list(image_names)

# ==============================================================================
# 3. è´Ÿé‡‡æ ·é€»è¾‘ (Batch æ„é€ )
# ==============================================================================
def create_itm_batch(images, captions, image_names, dataset):
    """
    ä¿®æ­£ç‰ˆï¼šæ„é€ çœŸæ­£çš„è´Ÿæ ·æœ¬ (Image A + Text B)
    """
    batch_size = len(images)
    
    # --- 1. æ­£æ ·æœ¬ (Image A + Text A) ---
    positive_images = list(images) 
    positive_texts = list(captions)
    positive_labels = [1] * batch_size
    
    # --- 2. è´Ÿæ ·æœ¬ (Image A + Text B) ---
    # ç­–ç•¥ï¼šå›¾ç‰‡è¿˜æ˜¯è¿™æ‰¹å›¾ç‰‡ï¼Œä½†æ˜¯æ–‡å­—æ¢æˆåˆ«äººçš„
    negative_images = list(images) # å›¾ç‰‡ä¸å˜ (Image A)
    negative_texts = [] # å‡†å¤‡å¡«å…¥é”™è¯¯çš„æ–‡å­— (Text B)
    
    for i in range(batch_size):
        current_image_name = image_names[i]
        
        # æ­»å¾ªç¯ç›´åˆ°æ‰¾åˆ°ä¸€ä¸ªâ€œåˆ«äººçš„â€æ–‡å­—
        while True:
            # éšæœºä»æ•°æ®é›†é‡ŒæŠ½ä¸€ä¸ªç´¢å¼•
            random_idx = random.randint(0, len(dataset.samples) - 1)
            other_image_name, other_caption = dataset.samples[random_idx]
            
            # åªè¦è¿™å¼ å›¾çš„åå­—å’Œå½“å‰å›¾ä¸ä¸€æ ·ï¼Œé‚£å®ƒçš„æ–‡å­—å°±æ˜¯â€œé”™è¯¯çš„â€
            if other_image_name != current_image_name:
                negative_texts.append(other_caption)
                break
    
    negative_labels = [0] * batch_size
    
    # --- 3. åˆå¹¶ ---
    all_images = positive_images + negative_images
    all_texts = positive_texts + negative_texts
    all_labels = positive_labels + negative_labels
    
    # --- 4. æ‰“ä¹± ---
    combined = list(zip(all_images, all_texts, all_labels))
    random.shuffle(combined)
    
    all_images, all_texts, all_labels = zip(*combined)
    
    return list(all_images), list(all_texts), torch.tensor(all_labels, dtype=torch.long)
# ==============================================================================
# 4. ä¸»è®­ç»ƒå¾ªç¯
# ==============================================================================
if __name__ == "__main__":
    args = {
        "model_name": "./instructblip-vicuna-7b",
        "data_root": "./flickr_30k",
        "batch_size": 32,
        "lr": 5e-5,
        "epochs": 10,
        "load_in_8bit": False,
        "fusion_bias": -3.0 # è®°å½•ä¸€ä¸‹ä½ çš„ç‰¹æ®Šåˆå§‹åŒ–å‚æ•°
    }
    
    # <--- ã€SwanLab æ–°å¢ã€‘2. åˆå§‹åŒ–å®éªŒ ---
    swanlab.init(
        project="InstructBlip-DualTower", # é¡¹ç›®å
        experiment_name="full-finetune-v1", # å®éªŒå
        config=args, # è®°å½•è¶…å‚æ•°
        description="Training ITM head + Visual Fusion module with Depth Anything V2"
    )
    # --- 1. é…ç½®è·¯å¾„ ---
    MODEL_NAME = "./instructblip-vicuna-7b" 
    DATA_ROOT = "./flickr_30k"
    IMAGE_ROOT = os.path.join(DATA_ROOT, "flickr30k-images")
    CAPTION_FILE = os.path.join(DATA_ROOT, "captions_clean.token")
    CHECKPOINT_DIR = "./checkpoints_itm_fusion" # æ”¹ä¸ªååŒºåˆ†ä¸€ä¸‹
    # RESUME_PATH = "./checkpoints_itm_fusion/checkpoint_step_10500.pth" 
    RESUME_PATH = ""  # ä¸åŠ è½½ï¼Œé‡æ–°è®­ç»ƒ
    # --- 2. æ˜¾å­˜ä¸ç²¾åº¦è®¾ç½® ---
    LOAD_IN_8BIT = False  # æ˜¾å­˜<24G æ—¶å»ºè®®å¼€å¯
    BATCH_SIZE = 32      # èåˆå±‚å¢åŠ äº†è®¡ç®—é‡ï¼Œå¯èƒ½éœ€è¦ç¨å¾®è°ƒå° Batch Size
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    os.makedirs(CHECKPOINT_DIR, exist_ok=True)

    print(f"Loading Processor from {MODEL_NAME}...")
    processor = InstructBlipProcessor.from_pretrained(MODEL_NAME)
    # Q-Former å¿…é¡»ä½¿ç”¨ BERT Tokenizer (è¿™æ˜¯ InstructBLIP çš„ç¡¬æ€§è¦æ±‚)
    qformer_tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

    print("Loading Dual-Tower Model...")
    # åŠ è½½æˆ‘ä»¬è‡ªå®šä¹‰çš„åŒå¡”æ¨¡å‹
    model = InstructBlipMultiTask.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.bfloat16, 
        load_in_8bit=LOAD_IN_8BIT,
        device_map="auto" if LOAD_IN_8BIT else None
    )
    
    if not LOAD_IN_8BIT:
        model.to(device)

    # --- 3. ã€æ ¸å¿ƒä¿®æ”¹ã€‘å‚æ•°å†»ç»“ä¸è§£å†» ---
    print("Configuring trainable parameters...")
    
    trainable_modules = ["itm_head", "visual_fusion"] # æˆ‘ä»¬è¦è®­ç»ƒçš„ä¸¤ä¸ªæ¨¡å—
    
    for name, param in model.named_parameters():
        # æ£€æŸ¥å‚æ•°åæ˜¯å¦åŒ…å«æˆ‘å€‘è¦è®­ç»ƒçš„æ¨¡å—å
        is_trainable = any(module_name in name for module_name in trainable_modules)
        
        if is_trainable:
            param.requires_grad = True
            # ã€é‡è¦ã€‘è®­ç»ƒçš„å±‚å»ºè®®è½¬å› FP32ï¼Œé˜²æ­¢ Loss NaN æˆ–æ¢¯åº¦ä¸‹æº¢
            param.data = param.data.to(torch.bfloat16) 
            print(f"  -> Unfrozen: {name}") 
        else:
            param.requires_grad = False
            
    # è®¡ç®—å‚æ•°é‡
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Total Trainable Parameters: {trainable_params / 1e6:.2f} M")
    
    if os.path.exists(RESUME_PATH):
        print(f"ğŸ”„æ­£åœ¨åŠ è½½æƒé‡: {RESUME_PATH} ...")
        
        # 1. è¯»å–æ–‡ä»¶
        checkpoint = torch.load(RESUME_PATH, map_location=device)
        
        # 2. åˆ†åˆ«åŠ è½½ visual_fusion å’Œ itm_head
        # æ³¨æ„ï¼šå› ä¸ºæˆ‘ä»¬ä¿å­˜çš„æ˜¯ä¸ªå­—å…¸ {'visual_fusion': ..., 'itm_head': ...}
        # æ‰€ä»¥ä¸èƒ½ç›´æ¥ model.load_state_dict(checkpoint)
        
        try:
            model.visual_fusion.load_state_dict(checkpoint['visual_fusion'])
            print("  âœ… Visual Fusion æƒé‡åŠ è½½æˆåŠŸ")
        except KeyError:
            print("  âš ï¸ è­¦å‘Š: Checkpoint ä¸­æœªæ‰¾åˆ° visual_fusion")
            
        try:
            model.itm_head.load_state_dict(checkpoint['itm_head'])
            print("  âœ… ITM Head æƒé‡åŠ è½½æˆåŠŸ")
        except KeyError:
            print("  âš ï¸ è­¦å‘Š: Checkpoint ä¸­æœªæ‰¾åˆ° itm_head")
            
        print("ğŸš€ æƒé‡åŠ è½½å®Œæ¯•ï¼Œå‡†å¤‡ç»§ç»­è®­ç»ƒï¼")
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ°è·¯å¾„ {RESUME_PATH}ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒï¼")
    # --- 5. ä¼˜åŒ–å™¨ ---
    # åªä¼ å…¥ requires_grad=True çš„å‚æ•°
    # ä¿®æ”¹ä¼˜åŒ–å™¨å®šä¹‰
    fusion_params = list(map(id, model.visual_fusion.parameters()))
    base_params = filter(lambda p: id(p) not in fusion_params and p.requires_grad, model.parameters())

    optimizer = torch.optim.AdamW([
        {'params': base_params, 'lr': 5e-5}, # Head ä¿æŒå° LR
        {'params': model.visual_fusion.parameters(), 'lr': 5e-4} # Fusion å±‚å¤§ LR (æ”¾å¤§10å€)
    ], weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000)
    
    # æ•°æ®åŠ è½½
    dataset = Flickr30kDataset(IMAGE_ROOT, CAPTION_FILE)
    dataloader = DataLoader(
        dataset, 
        batch_size=BATCH_SIZE, 
        shuffle=True, 
        num_workers=4, # é€‚å½“é™ä½ worker é˜²æ­¢å†…å­˜çˆ†ç‚¸
        collate_fn=collate_fn 
    )

    print("Start Dual-Tower ITM training...")
    
    criterion = nn.CrossEntropyLoss()
    num_epochs = 10
    save_every_steps = 500
    global_step = 0
    
    model.train() 

    for epoch in range(num_epochs):
        epoch_loss = 0
        epoch_acc = 0
        steps_in_epoch = 0
        
        for step, (images, captions, image_names) in enumerate(dataloader):
            # A. æ„é€ æ­£è´Ÿæ ·æœ¬ (Batch Size * 2)
            itm_images_pil, itm_texts, itm_labels = create_itm_batch(
                images, captions, image_names, dataset
            )
            itm_labels = itm_labels.to(device)
            
            # B. æ•°æ®é¢„å¤„ç†
            # å›¾ç‰‡ -> RGB Tensor
            image_inputs = processor(
                images=itm_images_pil,
                return_tensors="pt"
            ).to(device)
            
            # æ–‡æœ¬ -> Q-Former Token IDs
            text_inputs = qformer_tokenizer(
                itm_texts,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=32 
            ).to(device)
            
            # C. å‰å‘ä¼ æ’­
            optimizer.zero_grad()
            
            # è°ƒç”¨ forward_itm (å†…éƒ¨ä¼šè‡ªåŠ¨è°ƒç”¨ Depth backbone å’Œ Fusion)
            logits = model.forward_itm(
                pixel_values=image_inputs.pixel_values.to(dtype=torch.bfloat16),
                input_ids=text_inputs.input_ids,         
                attention_mask=text_inputs.attention_mask 
            )
            
            loss = criterion(logits, itm_labels)
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            # print(f"Gate Bias Grad: {model.visual_fusion.gate_net[-2].bias.grad}")
            optimizer.step()
            
            # D. ç»Ÿè®¡ä¸æ—¥å¿—
            preds = logits.argmax(dim=1)
            acc = (logits.argmax(dim=1) == itm_labels).float().mean().item()
            loss_val = loss.item()
            
            epoch_loss += loss_val
            epoch_acc += acc
            steps_in_epoch += 1
            global_step += 1
            swanlab.log({
                            "train/loss": loss_val,
                            "train/acc": acc,
                            "train/lr": optimizer.param_groups[0]['lr']
                        })
            if step % 100 == 0:
                # å– Batch é‡Œçš„ç¬¬ä¸€å¼ å›¾åšå±•ç¤º
                # è®°å½•ï¼šåŸå§‹å›¾ç‰‡ + æ–‡æœ¬ + çœŸå®æ ‡ç­¾ + é¢„æµ‹æ ‡ç­¾
                log_image = swanlab.Image(
                    itm_images_pil[0], 
                    caption=f"Text: {itm_texts[0]} | GT: {itm_labels[0]} | Pred: {preds[0].item()}"
                )
                swanlab.log({"val/visualization": log_image})
            if step % 10 == 0:
                print(f"Epoch [{epoch+1}/{num_epochs}], Step [{step}/{len(dataloader)}], "
                      f"Loss: {loss_val:.4f}, Acc: {acc:.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}")
                
                # ç›‘æ§ Gate çš„å€¼ (å¯é€‰ï¼Œè°ƒè¯•ç”¨)
                # æˆ‘ä»¬å¯ä»¥çœ‹çœ‹ Gate æ˜¯å¦ä» 0 å¼€å§‹é€æ¸å˜å¤§
                with torch.no_grad():
                   print(f"  Sample Gate Value: {model.visual_fusion.gate_net[-2].bias.data[0]:.4f} (Bias)")

            # --- 6. ã€æ ¸å¿ƒä¿®æ”¹ã€‘ä¿å­˜é€»è¾‘ ---
            if global_step % save_every_steps == 0:
                ckpt_path = os.path.join(CHECKPOINT_DIR, f"checkpoint_step_{global_step}.pth")
                
                # æˆ‘ä»¬éœ€è¦ä¿å­˜ä¸¤ä¸ªéƒ¨åˆ†ï¼šFusion Layer å’Œ ITM Head
                save_dict = {
                    "visual_fusion": model.visual_fusion.state_dict(),
                    "itm_head": model.itm_head.state_dict()
                }
                torch.save(save_dict, ckpt_path)
                print(f"Checkpoint saved -> {ckpt_path}")

        scheduler.step()
        
        avg_loss = epoch_loss / steps_in_epoch
        avg_acc = epoch_acc / steps_in_epoch
        print(f"=== Epoch {epoch+1} Finished. Avg Loss: {avg_loss:.4f}, Avg Acc: {avg_acc:.4f} ===")

    # ä¿å­˜æœ€ç»ˆç»“æœ
    final_path = os.path.join(CHECKPOINT_DIR, "final_dual_tower.pth")
    save_dict = {
        "visual_fusion": model.visual_fusion.state_dict(),
        "itm_head": model.itm_head.state_dict()
    }
    torch.save(save_dict, final_path)
    print(f"Training Done. Final weights saved to {final_path}")