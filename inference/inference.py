import os 
import torch
import torch.nn as nn
from PIL import Image
import requests
import numpy as np
from io import BytesIO
import sys
current_path = os.path.abspath(__file__)
inference_dir = os.path.dirname(current_path)
project_root = os.path.dirname(inference_dir)
sys.path.append(project_root)
from utils.utils import prepare_inputs_for_generate
# å¼•å…¥å®šä¹‰å¥½çš„æ¨¡å‹ç±»
try:
    from models.rvln import RvlnMultiTask
except ImportError:
    raise ImportError("è¯·ç¡®ä¿ models/rvln.py å­˜åœ¨ï¼Œå¹¶ä¸”å…¶ä¸­å®šä¹‰äº† RvlnMultiTask ç±»ã€‚")

from transformers import (
    InstructBlipProcessor,
    BertTokenizer
)

# ================= é…ç½®åŒºåŸŸ =================
# RVLN åˆå¹¶åçš„æƒé‡è·¯å¾„ (ç”¨äº Task 1 ç”Ÿæˆ)
RVLN_MODEL_PATH = "output/rvln_merged_final"
# åŸºç¡€ Vicuna è·¯å¾„ (ç”¨äºåŠ è½½ Processor)
BASE_PROCESSOR_PATH = "./instructblip-vicuna-7b"
# ITM / Stage1 æƒé‡è·¯å¾„ (ç”¨äº Task 2 ITM)
ITM_CHECKPOINT_PATH = "output/stage1_checkpoint/latest_checkpoint.pth"

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DTYPE = torch.float16 # æ¨ç†å»ºè®® fp16

# RVLN åºåˆ—å‚æ•°
HISTORY_LEN = 4
CURRENT_LEN = 1
TOTAL_LEN = 5
QUERY_TOKENS = 32

def load_combined_model():
    print(f"æ­£åœ¨åˆå§‹åŒ– (Device: {DEVICE}, Main Dtype: {DTYPE})...")

    # 1. åŠ è½½ Processor & Tokenizer
    # ä¼˜å…ˆå°è¯•ä»åˆå¹¶è·¯å¾„åŠ è½½ï¼Œå¤±è´¥åˆ™å›é€€åŸºç¡€è·¯å¾„
    try:
        processor = InstructBlipProcessor.from_pretrained(RVLN_MODEL_PATH)
    except:
        print(f"âš ï¸ æ— æ³•ä» {RVLN_MODEL_PATH} åŠ è½½ Processorï¼Œä½¿ç”¨åŸºç¡€è·¯å¾„...")
        processor = InstructBlipProcessor.from_pretrained(BASE_PROCESSOR_PATH)
    
    tokenizer = processor.tokenizer
    tokenizer.padding_side = "right"
    
    
    hist_id = tokenizer.convert_tokens_to_ids("<history>")
    curr_id = tokenizer.convert_tokens_to_ids("<current>")
    vocab_size = len(tokenizer)
    print(f"   -> Tokenizer IDs: <history>={hist_id}, <current>={curr_id}, Vocab={vocab_size}")

    # 3. åŠ è½½ ITM ä¸“ç”¨çš„ Q-Former Tokenizer
    qformer_tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

    # 4. åŠ è½½ RvlnMultiTask æ¨¡å‹
    print(f"åŠ è½½ RvlnMultiTask æ¨¡å‹: {RVLN_MODEL_PATH} ...")
    model = RvlnMultiTask.from_pretrained(
        RVLN_MODEL_PATH, 
        torch_dtype=DTYPE
    )
    model.to(DEVICE)
    model.eval()

    # 5. [å…³é”®] ID å¼ºåˆ¶åŒæ­¥ (é˜²æ­¢ç”Ÿæˆä¹±ç )
    print("ğŸ”§ æ‰§è¡Œ ID åŒæ­¥...")
    model.config.history_token_id = hist_id
    model.config.current_token_id = curr_id
    
    # Resize embedding å¦‚æœéœ€è¦
    if model.language_model.get_input_embeddings().weight.shape[0] < vocab_size:
        model.language_model.resize_token_embeddings(vocab_size)

    # 6. [å¯é€‰] åŠ è½½é¢å¤–çš„ ITM æƒé‡
    # å¦‚æœ merged_model é‡Œæ²¡æœ‰åŒ…å« stage 1 çš„ itm_head æƒé‡ï¼Œè¿™é‡Œæ‰‹åŠ¨åŠ è½½
    if os.path.exists(ITM_CHECKPOINT_PATH):
        print(f"ğŸ“¥ å‘ç° ITM æƒé‡: {ITM_CHECKPOINT_PATH}ï¼Œæ­£åœ¨åŠ è½½è¦†ç›–...")
        checkpoint = torch.load(ITM_CHECKPOINT_PATH, map_location="cpu")
        if 'depth_backbone' in checkpoint:
            model.depth_backbone.load_state_dict(checkpoint['depth_backbone'], strict=True)
        else :
            print("   âš ï¸ è­¦å‘Š: ITM æƒé‡ä¸­æœªæ‰¾åˆ° depth_backbone éƒ¨åˆ†ï¼Œè·³è¿‡è¯¥éƒ¨åˆ†åŠ è½½ã€‚")
        if 'visual_fusion' in checkpoint:
            model.visual_fusion.load_state_dict(checkpoint['visual_fusion'], strict=True)
        else :
            print("   âš ï¸ è­¦å‘Š: ITM æƒé‡ä¸­æœªæ‰¾åˆ° visual_fusion éƒ¨åˆ†ï¼Œè·³è¿‡è¯¥éƒ¨åˆ†åŠ è½½ã€‚")
        if 'itm_head' in checkpoint:
            model.itm_head.load_state_dict(checkpoint['itm_head'], strict=True)
        else :
            print("   âš ï¸ è­¦å‘Š: ITM æƒé‡ä¸­æœªæ‰¾åˆ° itm_head éƒ¨åˆ†ï¼Œè·³è¿‡è¯¥éƒ¨åˆ†åŠ è½½ã€‚")
        if 'qformer' in checkpoint:
            model.qformer.load_state_dict(checkpoint['qformer'], strict=True)
        else :
            print("   âš ï¸ è­¦å‘Š: ITM æƒé‡ä¸­æœªæ‰¾åˆ° qformer éƒ¨åˆ†ï¼Œè·³è¿‡è¯¥éƒ¨åˆ†åŠ è½½ã€‚")
        if 'query_tokens' in checkpoint:
            model.query_tokens.data = checkpoint['query_tokens'].data.to(DEVICE)
        else :
            print("   âš ï¸ è­¦å‘Š: ITM æƒé‡ä¸­æœªæ‰¾åˆ° query_tokens éƒ¨åˆ†ï¼Œè·³è¿‡è¯¥éƒ¨åˆ†åŠ è½½ã€‚")
        # åŠ è½½ ITM Head
        if 'itm_head' in checkpoint:
            try:
                # æ³¨æ„ï¼šRvlnMultiTask å¯èƒ½å°† itm_head æ”¾åœ¨äº†ä¸åŒä½ç½®ï¼Œè§†ä½ çš„ç±»å®šä¹‰è€Œå®š
                # è¿™é‡Œå‡è®¾ç»“æ„å…¼å®¹
                msg = model.itm_head.load_state_dict(checkpoint['itm_head'], strict=False)
                model.itm_head.to(dtype=DTYPE)
                print(f"   -> ITM Head åŠ è½½æˆåŠŸ: {msg}")
            except Exception as e:
                print(f"   âš ï¸ ITM Head åŠ è½½å¤±è´¥ (å¯èƒ½ç»“æ„ä¸åŒ¹é…): {e}")
        
        # æ·±åº¦æ¨¡å‹è½¬æ¢
        if hasattr(model, 'depth_model'):
            model.depth_model.to(dtype=torch.float32)

    return model, processor, qformer_tokenizer

def run_inference():
    # åˆå§‹åŒ–
    model, processor, qformer_tokenizer = load_combined_model()

    # å‡†å¤‡æµ‹è¯•æ•°æ® (é€šç”¨)
    img_path = "test_data/rgb.jpg"
    depth_path = "test_data/depth.jpg"
    raw_image = Image.open(img_path).convert("RGB")

    # å‡†å¤‡æ·±åº¦å›¾ (å¦‚æœæ²¡æœ‰ï¼Œç”¨çº¯é»‘æ›¿ä»£æµ‹è¯•)
    depth_image = Image.open(depth_path).convert("L")

    # =================================================
    # Task 1: RVLN å¯¼èˆªç”Ÿæˆ (æ›¿æ¢äº†åŸæ¥çš„ Text Generation)
    # =================================================
    print("\n" + "="*40)
    print("æµ‹è¯• 1: RVLN å¯¼èˆªæŒ‡ä»¤é¢„æµ‹")
    print("="*40)
    
    instruction = "go to the bedroom and the mirror is in front of you."
    
    # æ¨¡æ‹Ÿ RVLN è¾“å…¥é˜Ÿåˆ— (å‡è®¾åªæœ‰å½“å‰å¸§)
    rgb_queue = [raw_image]
    depth_queue = [depth_image]
    
    print(f"Instruction: {instruction}")
    
    # é¢„å¤„ç† (RVLN ä¸“ç”¨)
    rvln_inputs = prepare_inputs_for_generate(rgb_queue, depth_queue, instruction, processor, DEVICE)
    print("ğŸš€ RVLN ç”Ÿæˆä¸­...")
    with torch.no_grad():
        outputs = model.generate(
            pixel_values=rvln_inputs["pixel_values"],
            depth_pixel_values=rvln_inputs["depth_pixel_values"],
            qformer_input_ids=rvln_inputs["qformer_input_ids"],
            qformer_attention_mask=rvln_inputs["qformer_attention_mask"],
            input_ids=rvln_inputs["input_ids"],
            attention_mask=rvln_inputs["attention_mask"],
            max_new_tokens=100,
            do_sample=False
        )
    
    output_text = processor.batch_decode(outputs, skip_special_tokens=True)[0]
    
    print(f"ğŸ¤– RVLN Output: {output_text.strip()}")


    # =================================================
    # Task 2: å›¾æ–‡åŒ¹é… (ITM) (ä¿ç•™å¹¶é€‚é…)
    # =================================================
    print("\n" + "="*40)
    print("æµ‹è¯• 2: å›¾æ–‡åŒ¹é… (ITM)")
    print("="*40)
    
    test_texts = [
        "A photo of two cats sleeping on a sofa.", 
        "A red sports car driving on the highway", 
        instruction 
    ]
    
    
    print("æ­£åœ¨è®¡ç®— ITM åˆ†æ•°...")
    
    # 1. å‡†å¤‡å›¾åƒ Tensor (æ‰©å±•åˆ° 5 å¸§)
    itm_rgb_queue = [raw_image]
    itm_depth_queue = [depth_image] # æ·±åº¦å›¾ä¹Ÿéœ€è¦
    
    # å¤ç”¨å‡½æ•°æ‹¿åˆ° Tensor [1, 5, 3, H, W]
    dummy_input = prepare_inputs_for_generate(itm_rgb_queue, itm_depth_queue, "dummy", processor, DEVICE)
    pixel_values_5d = dummy_input["pixel_values"] # [1, 5, 3, H, W]
    depth_pixel_values_5d = dummy_input["depth_pixel_values"] # [1, 5, 3, H, W]
    
    # æ£€æŸ¥ NaN
    if torch.isnan(pixel_values_5d).any():
        print("è‡´å‘½é”™è¯¯: è¾“å…¥å›¾åƒ Tensor åŒ…å« NaNï¼")
        return

    # æ‰©å±• batch ç»´åº¦ä»¥åŒ¹é… text æ•°é‡
    pixel_values_expanded = pixel_values_5d.repeat(len(test_texts), 1, 1, 1, 1)
    depth_values_expanded = depth_pixel_values_5d.repeat(len(test_texts), 1, 1, 1, 1)
    # 2. å‡†å¤‡æ–‡æœ¬
    text_inputs = qformer_tokenizer(
        test_texts,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=32
    ).to(DEVICE)

    current_pixel_values = pixel_values_expanded[:, -1, :, :, :]
    current_depth_values = depth_values_expanded[:, -1, :, :, :]

    print(f"Input Shape for ITM: {current_pixel_values.shape} (Expected: [B, 3, H, W])")

    with torch.no_grad():
        # è°ƒç”¨ forward_itm
        logits = model.forward_itm(
            pixel_values=current_pixel_values, 
            depth_pixel_values=current_depth_values,
            input_ids=text_inputs.input_ids,
            attention_mask=text_inputs.attention_mask
        )
        probs = torch.softmax(logits, dim=1)
    
    print("\nåŒ¹é…ç»“æœ:")
    for i, text in enumerate(test_texts):
        score_match = probs[i][1].item()
        
        if np.isnan(score_match):
            bar_len = 0
            score_str = "NaN"
        else:
            bar_len = int(score_match * 20)
            score_str = f"{score_match:.6f}"
            
        bar = "â–ˆ" * bar_len + "â–‘" * (20 - bar_len)
        print(f"Text: '{text}'")
        print(f"Score: {score_str} | {bar}")
        print("-" * 30)

if __name__ == "__main__":
    run_inference()