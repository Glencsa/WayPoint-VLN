import os
import torch
import numpy as np
from PIL import Image
from transformers import InstructBlipProcessor
import sys
current_path = os.path.abspath(__file__)
inference_dir = os.path.dirname(current_path)
project_root = os.path.dirname(inference_dir)
sys.path.append(project_root)
from utils.utils import prepare_inputs_for_generate
try:
    from models.rvln import RvlnMultiTask
except ImportError:
    raise ImportError("è¯·ç¡®ä¿ models/rvln.py å­˜åœ¨ï¼Œå¹¶ä¸”å…¶ä¸­å®šä¹‰äº† RvlnMultiTask ç±»ã€‚")


CHECKPOINT_PATH = "output/rvln_merged_final"  
stage1_checkpoint = "output/stage1_checkpoint/latest_checkpoint.pth"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DTYPE = torch.float16  

def load_model():
    print(f"Loading model from: {CHECKPOINT_PATH}")
    processor = InstructBlipProcessor.from_pretrained(CHECKPOINT_PATH)
    tokenizer = processor.tokenizer
    tokenizer.padding_side = "right"
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    hist_id = tokenizer.convert_tokens_to_ids("<history>")
    curr_id = tokenizer.convert_tokens_to_ids("<current>")
    vocab_size = len(tokenizer)
    print(f"   -> Tokenizer IDs: <history>={hist_id}, <current>={curr_id}, Vocab={vocab_size}")
    # Load Model
    model = RvlnMultiTask.from_pretrained(
        CHECKPOINT_PATH,
        torch_dtype=DTYPE,
    ).to(DEVICE)
    if os.path.exists(stage1_checkpoint):
        print(f"ğŸ“¥ å‘ç° ITM æƒé‡: {stage1_checkpoint}ï¼Œæ­£åœ¨åŠ è½½è¦†ç›–...")
        checkpoint = torch.load(stage1_checkpoint, map_location="cpu")
        if 'depth_backbone' in checkpoint:
            model.depth_backbone.load_state_dict(checkpoint['depth_backbone'], strict=True)
        else :
            print("   âš ï¸ è­¦å‘Š: ITM æƒé‡ä¸­æœªæ‰¾åˆ° depth_backbone éƒ¨åˆ†ï¼Œè·³è¿‡è¯¥éƒ¨åˆ†åŠ è½½ã€‚")
        if 'visual_fusion' in checkpoint:
            model.visual_fusion.load_state_dict(checkpoint['visual_fusion'], strict=True)
        else :
            print("   âš ï¸ è­¦å‘Š: ITM æƒé‡ä¸­æœªæ‰¾åˆ° visual_fusion éƒ¨åˆ†ï¼Œè·³è¿‡è¯¥éƒ¨åˆ†åŠ è½½ã€‚")
        if 'itm_head' in checkpoint:
            model.itm_head.load_state_dict(checkpoint['itm_head'], strict=True)
        else :
            print("   âš ï¸ è­¦å‘Š: ITM æƒé‡ä¸­æœªæ‰¾åˆ° itm_head éƒ¨åˆ†ï¼Œè·³è¿‡è¯¥éƒ¨åˆ†åŠ è½½ã€‚")
        if 'qformer' in checkpoint:
            model.qformer.load_state_dict(checkpoint['qformer'], strict=True)
        else :
            print("   âš ï¸ è­¦å‘Š: ITM æƒé‡ä¸­æœªæ‰¾åˆ° qformer éƒ¨åˆ†ï¼Œè·³è¿‡è¯¥éƒ¨åˆ†åŠ è½½ã€‚")
        if 'query_tokens' in checkpoint:
            model.query_tokens.data = checkpoint['query_tokens'].data.to(DEVICE)
        else :
            print("   âš ï¸ è­¦å‘Š: ITM æƒé‡ä¸­æœªæ‰¾åˆ° query_tokens éƒ¨åˆ†ï¼Œè·³è¿‡è¯¥éƒ¨åˆ†åŠ è½½ã€‚")
    model.eval()
    # model_emb_size = model.language_model.get_input_embeddings().weight.shape[0]
    # print(f"   -> Model Embedding Size: {model_emb_size}")
    # model.language_model.resize_token_embeddings(len(tokenizer))
    print("Model loaded successfully!")
    return model, processor


def run_inference(model, processor, rgb_input, depth_input, instruction):
    """
    rgb_input: å¯ä»¥æ˜¯å•å¼ å›¾ç‰‡è·¯å¾„(str)ï¼Œä¹Ÿå¯ä»¥æ˜¯è·¯å¾„åˆ—è¡¨(list[str])
    depth_input: åŒä¸Š
    """
    # ç»Ÿä¸€è½¬ä¸º list æ ¼å¼æ–¹ä¾¿å¤„ç†
    if not isinstance(rgb_input, list):
        rgb_input = [rgb_input]
    if not isinstance(depth_input, list):
        depth_input = [depth_input]
        
    print(f"\nğŸ“¸ Processing sequence (len={len(rgb_input)})...")
    
    # 1. é¢„å¤„ç† (è‡ªåŠ¨è¡¥é½)
    inputs = prepare_inputs_for_generate(rgb_input, depth_input, instruction, processor, model.device)
    # print("input:"  , inputs)
    # 2. ç”Ÿæˆ
    print("ğŸš€ Generating...")
    with torch.no_grad():
        outputs = model.generate(
            pixel_values=inputs["pixel_values"],
            depth_pixel_values=inputs["depth_pixel_values"],
            qformer_input_ids=inputs["qformer_input_ids"],
            qformer_attention_mask=inputs["qformer_attention_mask"],
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=100,
            do_sample=False,
            repetition_penalty=1.0 
        )

    # 3. è§£ç 
    print("output:", outputs)
    output_text = processor.batch_decode(outputs, skip_special_tokens=True)[0]
    
    print("-" * 40)
    print(f"ğŸ“ Prediction: {output_text.strip()}")
    print("-" * 40)

if __name__ == "__main__":
    # åˆå§‹åŒ–
    model, processor = load_model()
    
    instruction = 'Walk past the foot of the bed and exit the bedroom through the double doors ahead of you. Once out of the bedroom take a quick dogleg to the left and enter the large room with a chandelier ahead of you.'
    
    # åœºæ™¯ 1: åªæœ‰å½“å‰ä¸€å¼ å›¾ (åˆšå¯åŠ¨)
    # ç³»ç»Ÿä¼šè‡ªåŠ¨è¡¥é½ä¸º: [é»‘, é»‘, é»‘, é»‘, Img1]
    rgb_1 = ["test_data/rgb.jpg"]
    depth_1 = ["test_data/depth.jpg"]
    run_inference(model, processor, rgb_1, depth_1, instruction)

    # # åœºæ™¯ 2: å·²ç»èµ°äº†å‡ æ­¥ (å†å²é˜Ÿåˆ—)
    # # ç³»ç»Ÿä¼šè‡ªåŠ¨å–æœ€å5å¼ : [Img1, Img2, Img3, Img4, Img5] (å‡è®¾ Img5 æ˜¯å½“å‰)
    # # è¿™é‡Œç”¨åŒä¸€ä¸ªå›¾æ¨¡æ‹Ÿå¤šå¸§
    # rgb_history = [rgb_1[0]] * 6  # æ¨¡æ‹Ÿæœ‰6å¼ å›¾
    # depth_history = [depth_1[0]] * 6
    # run_inference(model, processor, rgb_history, depth_history, instruction)