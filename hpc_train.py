import os
import torch
import torch.nn as nn
import torch.distributed as dist
import numpy as np 
import torch.nn.functional as F
from torch.utils.data import random_split
from transformers import (
    InstructBlipProcessor,
    InstructBlipConfig,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig
)
from peft import (
    LoraConfig,
    get_peft_model,
    TaskType
)
from models.rvln import RvlnMultiTask 
from data_utils import RvlnLoRADataset, DataCollatorForRvln
from utils import *

# ==========================================
# 3. è‡ªå®šä¹‰ Trainer (ç¡®ä¿ä¿å­˜ Embeddings)
# ==========================================
class CustomTrainer(Trainer):
    def save_model(self, output_dir=None, _internal_call=False):
        """é‡å†™ä¿å­˜é€»è¾‘ï¼Œç¡®ä¿ LoRA + Embeddings + Tokenizer éƒ½èƒ½è¢«ä¿å­˜"""
        if output_dir is None:
            output_dir = self.args.output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        # 1. ä¿å­˜ LoRA å’Œ modules_to_save (embed_tokens)
        super().save_model(output_dir, _internal_call)
        
        # 2. ä¿å­˜ Tokenizer
        if self.is_world_process_zero():
            self.tokenizer.save_pretrained(output_dir)
            
            print(f"Model (LoRA + Embeddings) saved to {output_dir}")

class WeightedTrainer(CustomTrainer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
        # --- åˆå§‹åŒ–ç›®æ ‡ Token é›†åˆ ---
        self.target_token_ids = set()
        
        # å®šä¹‰ä½ éœ€è¦åŠ æƒçš„æ•°å­—ï¼ˆå­—ç¬¦å½¢å¼ï¼‰
        # åŒ…æ‹¬ -1 å’Œ 0-8
        target_strings = [str(i) for i in range(9)] + ["-1", "-"] 
        
        # éå†è¯è¡¨ï¼Œæ‰¾åˆ°æ‰€æœ‰å¯èƒ½çš„ç¼–ç å½¢å¼
        vocab = self.tokenizer.get_vocab()
        
        # æ¨èï¼šç›´æ¥ç²¾å‡†æ·»åŠ  ID (ä»¥ Llama/Qwen ç­‰å¸¸ç”¨ Tokenizer ä¸ºä¾‹)
        # 1. çº¯æ•°å­—
        for i in range(9):
            # å°è¯•æ·»åŠ  "1", " 1" ç­‰å½¢å¼
            self.target_token_ids.add(self.tokenizer.convert_tokens_to_ids(str(i)))
            # æœ‰äº› tokenizer ä¼šæŠŠç©ºæ ¼åçš„æ•°å­—å•ç‹¬ä½œä¸ºä¸€ä¸ª token
            self.target_token_ids.add(self.tokenizer.convert_tokens_to_ids(" " + str(i)))
        
        # 2. å¤„ç†è´Ÿå· (å¯¹äº -1)
        self.target_token_ids.add(self.tokenizer.convert_tokens_to_ids("-"))
        self.target_token_ids.add(self.tokenizer.convert_tokens_to_ids(" -"))

        # ç§»é™¤å¯èƒ½å­˜åœ¨çš„ Unknown token ID
        if self.tokenizer.unk_token_id in self.target_token_ids:
            self.target_token_ids.remove(self.tokenizer.unk_token_id)
            
        print(f"WeightedTrainer: å·²æ¿€æ´»åŠ æƒ Token IDs: {self.target_token_ids}")

        # æƒé‡å€æ•°
        self.key_token_weight = 10.0

    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        """
        è‡ªå®šä¹‰ Loss è®¡ç®—ï¼Œå¯¹ç‰¹å®š Token è¿›è¡ŒåŠ æƒ
        """
        # 1. è·å– Labels å¹¶ç¡®ä¿ device æ­£ç¡®
        labels = inputs.get("labels")
        
        # 2. å‰å‘ä¼ æ’­
        outputs = model(**inputs)
        logits = outputs.get("logits")

        # 3. ç§»ä½ (Shift) æ“ä½œ - æ ¸å¿ƒæ­¥éª¤
        shift_logits = logits[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()

        # 4. å±•å¹³ (Flatten) ä»¥ä¾¿è®¡ç®— CrossEntropy
        batch_size, seq_len, vocab_size = shift_logits.shape
        flat_logits = shift_logits.view(-1, vocab_size)
        flat_labels = shift_labels.view(-1)

        # 5. è®¡ç®—æœªç¼©å‡ (Reduction='none') çš„ Loss
        loss_fct = nn.CrossEntropyLoss(reduction='none', ignore_index=-100)
        token_losses = loss_fct(flat_logits, flat_labels)

        # 6. æ„å»ºæƒé‡çŸ©é˜µ
        weights = torch.ones_like(token_losses)
        
        # 7. è¯†åˆ«ç›®æ ‡ Token å¹¶åŠ æƒ
        for target_id in self.target_token_ids:
            weights[flat_labels == target_id] = self.key_token_weight
            
        # 8. åº”ç”¨æƒé‡
        weighted_loss = token_losses * weights

        # 9. è®¡ç®—æœ€ç»ˆå¹³å‡ Loss
        active_elements = (flat_labels != -100).sum()
        
        if active_elements > 0:
            final_loss = weighted_loss.sum() / active_elements
        else:
            final_loss = weighted_loss.sum() # é˜²æ­¢é™¤ä»¥ 0

        return (final_loss, outputs) if return_outputs else final_loss


class ClassificationTrainer(CustomTrainer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.acc_buffer = []
        # [å·²ç§»é™¤] self.last_eval_visual_step = -1

    def generate_gaussian_target(self, labels, num_classes, sigma=1.0):
        """
        ç”Ÿæˆé«˜æ–¯è½¯æ ‡ç­¾
        """
        device = labels.device
        batch_size = labels.size(0)
        
        range_tensor = torch.arange(num_classes, device=device).unsqueeze(0).expand(batch_size, -1)
        target_tensor = labels.unsqueeze(1)
        
        distance = torch.abs(range_tensor - target_tensor)
        scores = torch.exp(- (distance.float() ** 2) / (2 * sigma ** 2))
        
        is_stop_token = (labels == 0) # [Batch]
        scores[:, 0] = 0.0
        
        probs = scores / (scores.sum(dim=1, keepdim=True) + 1e-9)
        
        one_hot = torch.zeros_like(probs)
        one_hot.scatter_(1, target_tensor, 1.0)
        
        final_targets = torch.where(is_stop_token.unsqueeze(1), one_hot, probs)
        
        return final_targets

    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        """
        Loss è®¡ç®— (é«˜æ–¯è½¯æ ‡ç­¾ç‰ˆ) + å‡†ç¡®ç‡ç´¯ç§¯
        """
        labels = inputs.get("class_labels")
        if labels is None:
            labels = inputs.get("labels")
            
        outputs = model(**inputs)
        logits = outputs.get("logits") # [Batch, Num_Classes]
        
        loss = None
        if logits is not None:
            # --- æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨ Soft Target Cross Entropy ---
            num_classes = logits.size(-1)
            soft_targets = self.generate_gaussian_target(labels, num_classes, sigma=1.5)
            
            log_probs = F.log_softmax(logits, dim=-1)
            
            loss_per_sample = -torch.sum(soft_targets * log_probs, dim=-1)
            loss = loss_per_sample.mean()

        # 4. è®¡ç®—å‡†ç¡®ç‡ (ä¿æŒä¸å˜ï¼Œå‡†ç¡®ç‡è¿˜æ˜¯çœ‹ç¡¬æŒ‡æ ‡)
        if logits is not None:
            with torch.no_grad():
                preds = torch.argmax(logits, dim=-1)
                micro_acc = (preds == labels).float().mean().item()
                
                if model.training:
                    self.acc_buffer.append(micro_acc)
                    if len(self.acc_buffer) >= self.args.gradient_accumulation_steps:
                        avg_acc = sum(self.acc_buffer) / len(self.acc_buffer)
                        self.log({"train/accuracy": avg_acc})
                        self.acc_buffer = []

        # [å·²ç§»é™¤] å¯è§†åŒ–ç›¸å…³è°ƒç”¨ self._handle_visualization(model, inputs, preds, labels)
        return (loss, outputs) if return_outputs else loss

    # [å·²ç§»é™¤] def _handle_visualization(self, model, inputs, preds, labels): ...
    # [å·²ç§»é™¤] def _log_visuals(self, inputs, preds, labels, prefix="Train"): ...
    # [å·²ç§»é™¤] def _tensor_to_pil(self, tensor, is_depth=False): ...


def main():
    # =================Configuration=================
    model_name_or_path = "./instructblip-vicuna-7b" 
    # Weight: Fusion, Q-Former, Depth
    stage1_checkpoint = "checkpoints/latest_checkpoint.pth"
    data_path = "/home/guanbin/scratch/dataset/r2r_dataset/rgb_images_r2r_train.json"
    output_dir = "./output/rvln_sft_llm"
    # è®­ç»ƒå‚æ•°
    batch_size = 4 
    grad_accumulation = 8 # ç¨å¾®åŠ å¤§ç´¯ç§¯ï¼Œæ¨¡æ‹Ÿæ›´å¤§ batch
    learning_rate = 5e-5  # SFT LLM å­¦ä¹ ç‡
    num_epochs = 3
    lora_rank = 32
    lora_alpha = 64
    
    # [å·²ç§»é™¤] SwanLab åˆå§‹åŒ–ä»£ç å—
    # swanlab.login(...)
    # swanlab.init(...)
    
    # =================1. Processor & Tokenizer=================
    print("Loading Processor...")
    processor = InstructBlipProcessor.from_pretrained(model_name_or_path)
    tokenizer = processor.tokenizer
    qformer_tokenizer = processor.qformer_tokenizer
    tokenizer.padding_side = "right"
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    # æ·»åŠ ç‰¹æ®Š Token
    special_tokens_dict = {'additional_special_tokens': ["<history>", "<current>"]}
    tokenizer.add_special_tokens(special_tokens_dict)
    
    history_token_id = tokenizer.convert_tokens_to_ids("<history>")
    current_token_id = tokenizer.convert_tokens_to_ids("<current>")

    # =================2. Model Initialization=================
    print("Loading Base Model...")
    config = InstructBlipConfig.from_pretrained(model_name_or_path)
    config.history_token_id = history_token_id
    config.current_token_id = current_token_id

    # åŠ è½½åŸºç¡€æ¨¡å‹
    model = RvlnMultiTask.from_pretrained(
        model_name_or_path,
        config=config,
        torch_dtype=torch.bfloat16
    )

    # è°ƒæ•´ Embedding å¤§å°
    model.language_model.resize_token_embeddings(len(tokenizer))

    # =================3. [å…³é”®] åŠ è½½ Stage 1 è®­ç»ƒå¥½çš„æƒé‡=================
    if os.path.exists(stage1_checkpoint):
        print(f"ğŸ“¥ Loading Stage 1 Checkpoint from: {stage1_checkpoint}")
        ckpt = torch.load(stage1_checkpoint, map_location="cpu")
        
        msg = model.load_state_dict(ckpt, strict=False) 
        print(f"Checkpoint Load Status: {msg}")
        
        if 'visual_fusion' in ckpt: print(" - Visual Fusion Loaded âœ…")
        if 'qformer' in ckpt: print(" - Q-Former Loaded âœ…")
        if 'depth_backbone' in ckpt: print(" - Depth Backbone Loaded âœ…")
    else:
        print("âŒ Warning: Stage 1 checkpoint not found! Training from scratch (Not Recommended).")

    # =================4. Freeze & LoRA Setup=================
    
    # 4.1 å…¨å±€å†»ç»“
    for param in model.parameters():
        param.requires_grad = False
        
    # 4.2 é…ç½® LoRA (é’ˆå¯¹ LLM)
    peft_config = LoraConfig(
        r=lora_rank,
        lora_alpha=lora_alpha,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type=TaskType.CAUSAL_LM,
        modules_to_save=["embed_tokens", "lm_head"]# "score_head" 
    )
    
    print("Applying LoRA to LLM...")
    model.language_model = get_peft_model(model.language_model, peft_config)
    
    print_trainable_parameters(model)

    # ================= 5. Data Setup (å…³é”®ä¿®æ”¹ï¼šåˆ’åˆ†éªŒè¯é›†) =================
    print("Loading Full Dataset...")
    full_dataset = RvlnLoRADataset(
        data_path=data_path,
        processor=processor,
        tokenizer=tokenizer,
        image_root="", 
        history_len=4,
        current_len=1
    )
    
    val_ratio = 0.01  # 1% åšéªŒè¯ï¼Œ99% è®­ç»ƒ
    val_size = int(len(full_dataset) * val_ratio)
    train_size = len(full_dataset) - val_size
    
    print(f"Splitting Dataset: Total={len(full_dataset)} | Train={train_size} | Val={val_size}")
    
    # generatorç”¨äºå›ºå®šéšæœºç§å­ï¼Œä¿è¯æ¯æ¬¡åˆ‡åˆ†ä¸€æ ·ï¼Œæ–¹ä¾¿å¤ç°
    train_dataset, eval_dataset = random_split(
        full_dataset, 
        [train_size, val_size],
        generator=torch.Generator().manual_seed(42) 
    )

    collator = DataCollatorForRvln(
        processor=processor,
        tokenizer=tokenizer,
        qformer_tokenizer=qformer_tokenizer
    )

    # ================= 6. Trainer Setup =================
    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=batch_size,
        gradient_accumulation_steps=grad_accumulation,
        learning_rate=learning_rate,
        num_train_epochs=num_epochs,
        fp16=False,
        bf16=True,
        deepspeed="./ds_config_zero2_1.json",
        remove_unused_columns=False,
        report_to="none", 
        evaluation_strategy="steps",   
        eval_steps=1000,                
        per_device_eval_batch_size=batch_size, 
        save_strategy="steps",         
        save_steps=2000,                
        save_total_limit=2,            
        load_best_model_at_end=True,   
        metric_for_best_model="loss",  
        greater_is_better=False,       
        logging_steps=4,
        dataloader_num_workers=16,
        dataloader_pin_memory=True,
        tf32=True,
        gradient_checkpointing=True,   
        gradient_checkpointing_kwargs={'use_reentrant': False},
    )

    # ä½¿ç”¨è‡ªå®šä¹‰ Trainer
    trainer = WeightedTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=collator,
        processing_class=tokenizer,
        compute_metrics=compute_metrics,
        callbacks=[], # [å·²ç§»é™¤] ç§»é™¤äº† SwanLabCallback
        preprocess_logits_for_metrics=preprocess_logits_for_metrics
    )

    trainer.train()
    trainer.accelerator.wait_for_everyone()
    
    # ä»…ç”±ä¸»è¿›ç¨‹è§¦å‘ä¿å­˜é€»è¾‘
    if trainer.is_world_process_zero():
        trainer.save_model(output_dir)
    if dist.is_initialized():
        dist.destroy_process_group()

if __name__ == "__main__":
    main()
