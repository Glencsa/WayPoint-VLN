import os
import torch
import torch.nn as nn
import torch.distributed as dist
import numpy as np 
import torch.nn.functional as F
from torch.utils.data import random_split
from transformers import (
    InstructBlipProcessor,
    InstructBlipConfig,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig
)
from peft import (
    LoraConfig,
    get_peft_model,
    TaskType
)
from models.rvln import RvlnMultiTask 
from utils.data_utils import RvlnLoRADataset, DataCollatorForRvln
from utils.utils import *

class WeightedTrainer(Trainer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
        # ==================== 1. åˆå§‹åŒ– Token æ˜ å°„ä¸è¶…å‚æ•° ====================
        self.target_token_ids = set() # ç”¨äºåŸºç¡€åŠ æƒ Mask (åŒ…å« -1)
        self.id_to_value = {}         # ç”¨äºè·ç¦»è®¡ç®— (åªåŒ…å« 0-8)
        self.digit_canonical_ids = [] # å­˜å‚¨ 0-8 çš„æ ‡å‡† Token IDï¼Œç”¨äºæå– Soft Logits
        
        # --- è¶…å‚æ•°è®¾ç½® ---
        self.key_token_weight = 1.0  # ç¡¬æ ‡ç­¾æƒé‡ (åšå¯¹äº†å¥–åŠ±å¤§)
        self.soft_loss_weight = 5.0   # è½¯æ ‡ç­¾æƒé‡ (æ§åˆ¶è·ç¦»æƒ©ç½šçš„åŠ›åº¦)
        self.sigma = 2.0              # é«˜æ–¯åˆ†å¸ƒæ ‡å‡†å·® (è¶Šå¤§è¶Šå®½å®¹)
        # --- A. æ³¨å†Œæ•°å­— 0-8 (å‚ä¸é«˜æ–¯è®¡ç®—) ---
        for i in range(9):
            s = str(i)
            # è·å–è¯¥æ•°å­—çš„æ‰€æœ‰å¯èƒ½ Token ID (ä¾‹å¦‚ "1", " 1")
            ids = [
                self.tokenizer.convert_tokens_to_ids(s),
                self.tokenizer.convert_tokens_to_ids(" " + s)
            ]
            
            # è®°å½•ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„ ID ä½œä¸ºè¯¥æ•°å­—çš„"ä»£è¡¨"ï¼Œç”¨äºæå– Logits è®¡ç®— Soft Loss
            # (é€šå¸¸ tokenizer çš„ç¬¬ä¸€ä¸ªç»“æœå°±æ˜¯æœ€å¸¸ç”¨çš„)
            canonical_added = False
            
            for tid in ids:
                if tid != self.tokenizer.unk_token_id:
                    self.target_token_ids.add(tid)
                    self.id_to_value[tid] = i  # å»ºç«‹ ID -> æ•´æ•°å€¼ çš„æ˜ å°„
                    
                    if not canonical_added:
                        self.digit_canonical_ids.append(tid)
                        canonical_added = True
        
        # ç¡®ä¿æˆ‘ä»¬æ”¶é›†é½äº† 0-8 çš„ä»£è¡¨ IDï¼Œå¦åˆ™æ— æ³•è¿›è¡Œ Softmax è®¡ç®—
        if len(self.digit_canonical_ids) != 9:
            print("âš ï¸ Warning: æ— æ³•æ‰¾åˆ° 0-8 çš„å®Œæ•´ Token IDï¼Œè½¯æ ‡ç­¾é€»è¾‘å¯èƒ½å—æŸã€‚")

        # --- B. æ³¨å†Œè´Ÿå·/-1 (åªåŠ æƒï¼Œä¸å‚ä¸é«˜æ–¯) ---
        # -1 ä»£è¡¨ Stopï¼Œå®ƒåœ¨ç©ºé—´ä¸Šæ²¡æœ‰"é‚»å±…"ï¼Œæ‰€ä»¥åªåšç¡¬åˆ†ç±»
        neg_ids = [
            self.tokenizer.convert_tokens_to_ids("-"),
            self.tokenizer.convert_tokens_to_ids(" -"),
            self.tokenizer.convert_tokens_to_ids("-1"),
            self.tokenizer.convert_tokens_to_ids(" -1")
        ]
        for tid in neg_ids:
            if tid != self.tokenizer.unk_token_id:
                self.target_token_ids.add(tid)
                # æ³¨æ„ï¼šä¸åœ¨ id_to_value ä¸­æ³¨å†Œ

        # æ‰“å°æ—¥å¿—ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
        if self.is_world_process_zero():
            print(f"WeightedTrainer Ready:")
            print(f"  - Hard Weighted Tokens: {len(self.target_token_ids)}")
            print(f"  - Distance Aware Tokens: 0-8 (Sigma={self.sigma})")

    def generate_gaussian_target(self, gt_values, num_classes=9):
        """
        ç”Ÿæˆé«˜æ–¯åˆ†å¸ƒç›®æ ‡
        gt_values: [Batch] çœŸå®çš„æ•°å­—å€¼ (0-8)
        """
        device = gt_values.device
        # åˆ›å»º [Batch, 9] çš„çŸ©é˜µï¼Œæ¯ä¸€è¡Œéƒ½æ˜¯ 0,1,2...8
        target_indices = torch.arange(num_classes, device=device).expand(len(gt_values), -1)
        # æ‰©å±• GT: [Batch, 1] -> [Batch, 9]
        gt_expand = gt_values.unsqueeze(1).expand(-1, num_classes)
        
        # è®¡ç®—è·ç¦»å¹³æ–¹
        distance = (target_indices - gt_expand).float() ** 2
        
        # é«˜æ–¯å…¬å¼: exp(-dist / 2*sigma^2)
        scores = torch.exp(-distance / (2 * self.sigma ** 2))
        
        # å½’ä¸€åŒ– (Sum = 1)ï¼Œè¿™å°±å˜æˆäº†ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒ
        probs = scores / scores.sum(dim=1, keepdim=True)
        return probs

    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        """
        Loss = Hard_Weighted_CE + Alpha * Soft_Gaussian_KL
        """
        # 1. è·å– Labels
        labels = inputs.get("labels")
        
        # 2. å‰å‘ä¼ æ’­
        outputs = model(**inputs)
        logits = outputs.get("logits")

        # 3. Shift æ“ä½œ (å¯¹é½ Logits å’Œ Labels)
        shift_logits = logits[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()

        # 4. å±•å¹³
        batch_size, seq_len, vocab_size = shift_logits.shape
        flat_logits = shift_logits.view(-1, vocab_size)
        flat_labels = shift_labels.view(-1)

        # ==================== Part 1: åŸºç¡€åŠ æƒ Loss (Hard Target) ====================
        # è®¡ç®—æ‰€æœ‰ Token çš„ CrossEntropy
        loss_fct = nn.CrossEntropyLoss(reduction='none', ignore_index=-100)
        token_losses = loss_fct(flat_logits, flat_labels)

        # æ„å»ºæƒé‡çŸ©é˜µ
        weights = torch.ones_like(token_losses)
        
        # æ ‡è®°å“ªäº›ä½ç½®æ˜¯éœ€è¦è®¡ç®—è·ç¦»çš„æ•°å­— (0-8)
        ordinal_mask = torch.zeros_like(token_losses, dtype=torch.bool)
        # å­˜å‚¨è¿™äº›ä½ç½®å¯¹åº”çš„çœŸå®æ•´æ•°å€¼
        ordinal_gt_values = torch.zeros_like(flat_labels, dtype=torch.long)

        # åº”ç”¨æƒé‡å¹¶è¯†åˆ«æ•°å­—
        # (è¿™é‡Œä¸ºäº†ä»£ç æ¸…æ™°ä½¿ç”¨äº†å¾ªç¯ï¼ŒToken åªæœ‰åå‡ ä¸ªï¼Œå¼€é”€å¯å¿½ç•¥)
        for target_id in self.target_token_ids:
            is_target = (flat_labels == target_id)
            # åŠ æƒ
            weights[is_target] = self.key_token_weight
            
            # å¦‚æœæ˜¯ 0-8ï¼ŒåŠ å…¥ Soft Loss è®¡ç®—é˜Ÿåˆ—
            if target_id in self.id_to_value:
                ordinal_mask |= is_target
                # è®°å½•è¯¥ Token ID å¯¹åº”çš„æ•´æ•°å€¼ (ä¾‹å¦‚ ID 299 -> Value 8)
                ordinal_gt_values[is_target] = self.id_to_value[target_id]

        weighted_loss = token_losses * weights
        
        # è®¡ç®—å¹³å‡ Hard Loss
        active_elements = (flat_labels != -100).sum()
        base_loss = weighted_loss.sum() / (active_elements + 1e-6)

        # ==================== Part 2: è·ç¦»æ„ŸçŸ¥ Loss (Soft Target) ====================
        soft_loss = torch.tensor(0.0, device=flat_logits.device)
        
        if ordinal_mask.any():
            # 1. å–å‡ºå±äºæ•°å­—çš„æ ·æœ¬çš„ Logits
            # æˆ‘ä»¬åªå…³å¿ƒæ¨¡å‹åœ¨ 0-8 è¿™ 9 ä¸ª Token ä¸Šçš„è¡¨ç°
            # digit_canonical_ids æ˜¯æˆ‘ä»¬é¢„å…ˆå­˜å¥½çš„ [id_0, id_1, ..., id_8]
            digit_ids_tensor = torch.tensor(self.digit_canonical_ids, device=flat_logits.device)
            
            # æå– Mask å¯¹åº”çš„ Logits è¡Œï¼Œä¸”åªæå– 9 ä¸ªæ•°å­—åˆ— -> [N_ordinal, 9]
            subset_logits = flat_logits[ordinal_mask][:, digit_ids_tensor]
            
            # 2. è®¡ç®— Log Softmax (æ¨¡å‹é¢„æµ‹åˆ†å¸ƒ)
            subset_log_probs = F.log_softmax(subset_logits, dim=-1)
            
            # 3. ç”Ÿæˆé«˜æ–¯ç›®æ ‡åˆ†å¸ƒ (Targetåˆ†å¸ƒ) -> [N_ordinal, 9]
            subset_gt = ordinal_gt_values[ordinal_mask]
            soft_targets = self.generate_gaussian_target(subset_gt, num_classes=9)
            
            # 4. è®¡ç®— KL æ•£åº¦ (KLDiv = -Sum(P_target * log P_pred))
            # è¡¡é‡æ¨¡å‹åˆ†å¸ƒä¸é«˜æ–¯åˆ†å¸ƒçš„å·®å¼‚
            kl_loss = F.kl_div(subset_log_probs, soft_targets, reduction='batchmean')
            
            soft_loss = kl_loss

        # ==================== Part 3: æ€» Loss ====================
        final_loss = base_loss + self.soft_loss_weight * soft_loss

        return (final_loss, outputs) if return_outputs else final_loss

    def save_model(self, output_dir=None, _internal_call=False):
            """
            è‡ªå®šä¹‰ä¿å­˜é€»è¾‘ï¼šé’ˆå¯¹åµŒå¥— LoRA ç»“æ„ (Rvln -> LLM -> LoRA)
            """
            if output_dir is None:
                output_dir = self.args.output_dir
            os.makedirs(output_dir, exist_ok=True)
            
            # --- å…³é”®æ­¥éª¤ï¼šè·å–è¢« Unwrap çš„æ¨¡å‹ ---
            # å¦‚æœä½¿ç”¨äº† DeepSpeed æˆ– DDPï¼Œæœ€å¤–å±‚ä¼šè¢« wrapï¼Œéœ€è¦å…ˆå‰¥ç¦»
            model_to_save = self.model
            if hasattr(model_to_save, "module"):
                model_to_save = model_to_save.module
                
            # --- å…³é”®æ­¥éª¤ï¼šå®šä½ LoRA æ ¸å¿ƒ ---
            # ä½ çš„ LoRA æ˜¯åŠ åœ¨ model.language_model ä¸Šçš„
            # è¿™é‡Œçš„ peft_model å°±æ˜¯é‚£ä¸ªè¢« get_peft_model åŒ…è£¹çš„å¯¹è±¡
            peft_model = model_to_save.language_model
            
            # ä»…åœ¨ä¸»è¿›ç¨‹æ‰§è¡Œä¿å­˜æ“ä½œ
            if self.is_world_process_zero():
                print(f"ğŸ’¾ Saving LoRA adapters and trained modules to {output_dir}...")
                
                # 1. ä¿å­˜ LoRA æƒé‡ + modules_to_save (embed_tokens, lm_head)
                # PEFT åº“ä¼šè‡ªåŠ¨å¤„ç† modules_to_saveï¼Œå°†å®ƒä»¬å’Œ adapter ä¸€èµ·å­˜ä¸‹æ¥
                peft_model.save_pretrained(output_dir)
                
                # 2. ä¿å­˜ Tokenizer
                saver = getattr(self, "processing_class", None) or getattr(self, "tokenizer", None)
                if saver:
                    saver.save_pretrained(output_dir)
                
                # 3. ä¿å­˜ Config (å¯é€‰ï¼Œæ–¹ä¾¿æŸ¥çœ‹)
                peft_model.config.save_pretrained(output_dir)

                print(f"âœ… Model components saved successfully.")
def main():
    # =================Configuration=================
    model_name_or_path = "./instructblip-vicuna-7b" 
    # Weight: Fusion, Q-Former, Depth
    stage1_checkpoint = "checkpoints/latest_checkpoint.pth"
    data_path = "/home/guanbin/scratch/dataset/r2r_dataset/rgb_images_r2r_train.json"
    output_dir = "./output/rvln_sft_llm_new"
    # è®­ç»ƒå‚æ•°
    batch_size = 4 
    grad_accumulation = 8 # ç¨å¾®åŠ å¤§ç´¯ç§¯ï¼Œæ¨¡æ‹Ÿæ›´å¤§ batch
    learning_rate = 2e-4  # SFT LLM å­¦ä¹ ç‡
    num_epochs = 10
    lora_rank = 32
    lora_alpha = 64
    
    
    # =================1. Processor & Tokenizer=================
    print("Loading Processor...")
    processor = InstructBlipProcessor.from_pretrained(model_name_or_path)
    tokenizer = processor.tokenizer
    tokenizer.padding_side = "right"
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    # æ·»åŠ ç‰¹æ®Š Token
    special_tokens_dict = {'additional_special_tokens': ["<history>", "<current>"]}
    tokenizer.add_special_tokens(special_tokens_dict)
    
    history_token_id = tokenizer.convert_tokens_to_ids("<history>")
    current_token_id = tokenizer.convert_tokens_to_ids("<current>")

    # =================2. Model Initialization=================
    print("Loading Base Model...")
    config = InstructBlipConfig.from_pretrained(model_name_or_path)
    config.history_token_id = history_token_id
    config.current_token_id = current_token_id

    # åŠ è½½åŸºç¡€æ¨¡å‹
    model = RvlnMultiTask.from_pretrained(
        model_name_or_path,
        config=config,
        torch_dtype=torch.bfloat16
    )

    # è°ƒæ•´ Embedding å¤§å°
    model.language_model.resize_token_embeddings(len(tokenizer))

    # =================3. [å…³é”®] åŠ è½½ Stage 1 è®­ç»ƒå¥½çš„æƒé‡=================
    if os.path.exists(stage1_checkpoint):
        print(f"ğŸ“¥ Loading Stage 1 Checkpoint from: {stage1_checkpoint}")
        ckpt = torch.load(stage1_checkpoint, map_location="cpu")
        
        msg = model.load_state_dict(ckpt, strict=False) 
        print(f"Checkpoint Load Status: {msg}")
        
        if 'visual_fusion' in ckpt: print(" - Visual Fusion Loaded âœ…")
        if 'qformer' in ckpt: print(" - Q-Former Loaded âœ…")
        if 'depth_backbone' in ckpt: print(" - Depth Backbone Loaded âœ…")
    else:
        print("âŒ Warning: Stage 1 checkpoint not found! Training from scratch (Not Recommended).")

    # =================4. Freeze & LoRA Setup=================
    
    # 4.1 å…¨å±€å†»ç»“
    for param in model.parameters():
        param.requires_grad = False
        
    # 4.2 é…ç½® LoRA (é’ˆå¯¹ LLM)
    peft_config = LoraConfig(
        r=lora_rank,
        lora_alpha=lora_alpha,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type=TaskType.CAUSAL_LM,
        modules_to_save=["embed_tokens", "lm_head"]# "score_head" 
    )
    
    print("Applying LoRA to LLM...")
    model.language_model = get_peft_model(model.language_model, peft_config)
    
    print_trainable_parameters(model)

    # ================= 5. Data Setup (å…³é”®ä¿®æ”¹ï¼šåˆ’åˆ†éªŒè¯é›†) =================
    print("Loading Full Dataset...")
    full_dataset = RvlnLoRADataset(
        data_path=data_path,
        processor=processor,
        tokenizer=tokenizer,
        image_root="", 
        history_len=4,
        current_len=1
    )
    
    val_ratio = 0.1  # 10% åšéªŒè¯ï¼Œ90% è®­ç»ƒ
    val_size = int(len(full_dataset) * val_ratio)
    train_size = len(full_dataset) - val_size
    
    print(f"Splitting Dataset: Total={len(full_dataset)} | Train={train_size} | Val={val_size}")
    
    # generatorç”¨äºå›ºå®šéšæœºç§å­ï¼Œä¿è¯æ¯æ¬¡åˆ‡åˆ†ä¸€æ ·ï¼Œæ–¹ä¾¿å¤ç°
    train_dataset, eval_dataset = random_split(
        full_dataset, 
        [train_size, val_size],
        generator=torch.Generator().manual_seed(42) 
    )

    collator = DataCollatorForRvln(
        processor=processor,
        tokenizer=tokenizer
    )

    # ================= 6. Trainer Setup =================
    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=batch_size,
        gradient_accumulation_steps=grad_accumulation,
        learning_rate=learning_rate,
        warmup_ratio=0.03,
        num_train_epochs=num_epochs,
        fp16=False,
        bf16=True,
        deepspeed="./config/ds_config_zero2_1.json",
        remove_unused_columns=False,
        report_to="none", 
        evaluation_strategy="steps",   
        eval_steps=1000,                
        per_device_eval_batch_size=batch_size, 
        save_strategy="steps",         
        save_steps=2000,                
        save_total_limit=2,            
        load_best_model_at_end=True,   
        metric_for_best_model="loss",  
        greater_is_better=False,       
        logging_steps=4,
        dataloader_num_workers=16,
        dataloader_pin_memory=True,
        tf32=True,
        gradient_checkpointing=True,   
        gradient_checkpointing_kwargs={'use_reentrant': False},
    )

    # ä½¿ç”¨è‡ªå®šä¹‰ Trainer
    trainer = WeightedTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=collator,
        processing_class=tokenizer,
        compute_metrics=compute_metrics,
        callbacks=[],
        preprocess_logits_for_metrics=preprocess_logits_for_metrics
    )

    trainer.train()
    trainer.accelerator.wait_for_everyone()
    
    # ================= 7. Save Adapter Only (å¸¸è§„ä¿å­˜ LoRAï¼Œä¸åˆå¹¶) =================
    # ä»…ä¸»è¿›ç¨‹æ‰§è¡Œä¿å­˜ï¼Œé¿å…å¤šè¿›ç¨‹å†™å…¥å†²çª
    if trainer.is_world_process_zero():
        print("â³ Starting Save process (Adapter Only)...")
        
        # 1. å®šä¹‰ä¿å­˜è·¯å¾„ (å»ºè®®å•ç‹¬ä¸€ä¸ªå­æ–‡ä»¶å¤¹ï¼Œæ¸…æ™°æ˜äº†)
        final_adapter_dir = os.path.join(output_dir, "final_adapter")
        os.makedirs(final_adapter_dir, exist_ok=True)

        # 2. è·å–æ¨¡å‹æœ¬ä½“ (å‰¥ç¦» DeepSpeed/DDP çš„å°è£…)
        model_to_save = trainer.model
        if hasattr(model_to_save, "module"):
            model_to_save = model_to_save.module

        # 3. å…³é”®æ­¥éª¤ï¼šå®šä½ LoRA æ¨¡å—
        # ä½ çš„ LoRA æ˜¯åŠ åœ¨ model.language_model ä¸Šçš„ï¼Œå®ƒæ˜¯ä¸€ä¸ª PeftModel å¯¹è±¡
        peft_model = model_to_save.language_model
        
        # 4. ä¿å­˜ LoRA æƒé‡
        # PEFT åº“ä¼šè‡ªåŠ¨æ£€æµ‹ config ä¸­çš„ modules_to_save (embed_tokens, lm_head)
        # å¹¶å°†å®ƒä»¬ä¸ lora æƒé‡ä¸€èµ·ä¿å­˜åˆ° adapter_model.safetensors ä¸­
        print(f"   - Saving LoRA adapters and trainable modules to {final_adapter_dir}...")
        peft_model.save_pretrained(final_adapter_dir)
        
        # 5. ä¿å­˜ Tokenizer
        # ç¡®ä¿æ¨ç†æ—¶ä½¿ç”¨çš„ tokenizer ä¸è®­ç»ƒæ—¶ä¸€è‡´
        print("   - Saving Tokenizer...")
        tokenizer.save_pretrained(final_adapter_dir)
        
        # 6. ä¿å­˜ LoRA Config (åŒ…å« rank, alpha, base_model_path ç­‰ä¿¡æ¯)
        peft_model.config.save_pretrained(final_adapter_dir)

        print(f"âœ… Adapter saved successfully! Path: {final_adapter_dir}")
        print("   (You can load this with PeftModel.from_pretrained over the base model)")

    if dist.is_initialized():
        dist.destroy_process_group()

if __name__ == "__main__":
    main()
