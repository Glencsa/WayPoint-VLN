import os 
import torch
import torch.nn as nn
from PIL import Image
import requests
from io import BytesIO
import numpy as np # æ–°å¢ numpy ç”¨äºæ£€æŸ¥ NaN

# å¼•å…¥å®šä¹‰å¥½çš„æ¨¡å‹ç±»
from models.InstructBlip import InstructBlipMultiTask
from transformers import (
    InstructBlipProcessor,
    BertTokenizer
)

def run_inference():

    MODEL_ID = "./instructblip-vicuna-7b"
    CHECKPOINT_PATH = "./checkpoints_itm_cross_attn/best_checkpoint.pth" 
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
    print(f"æ­£åœ¨åˆå§‹åŒ– (Device: {DEVICE}, Main Dtype: {DTYPE})...")

    # Load Model and Processor
    processor = InstructBlipProcessor.from_pretrained(MODEL_ID)
    qformer_tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
    print("åŠ è½½ InstructBlipMultiTask åŸºç¡€æ¨¡å‹...")
    model = InstructBlipMultiTask.from_pretrained(
        MODEL_ID, 
        torch_dtype=DTYPE
    )
    model.to(DEVICE)
    if hasattr(model, 'depth_model'):
        model.depth_model.to(dtype=torch.float32)
        print("Depth Model å·²å¼ºåˆ¶è½¬æ¢ä¸º Float32")
    else:
        print("è­¦å‘Šï¼šæœªæ£€æµ‹åˆ° depth_modelï¼Œè¯·æ£€æŸ¥æ¨¡å‹å®šä¹‰ï¼")
    if os.path.exists(CHECKPOINT_PATH):
        print(f"ğŸ“¥ å‘ç°è®­ç»ƒæƒé‡: {CHECKPOINT_PATH}ï¼Œæ­£åœ¨åŠ è½½...")
        checkpoint = torch.load(CHECKPOINT_PATH, map_location="cpu") # å…ˆåŠ è½½åˆ° CPU é˜²æ­¢æ˜¾å­˜æ³¢åŠ¨
        try:
            model.visual_fusion.load_state_dict(checkpoint['visual_fusion'], strict=True)
            model.visual_fusion.to(device=DEVICE, dtype=DTYPE) 
            print(f"Visual Fusion åŠ è½½æˆåŠŸ")
        except KeyError:
            print("é”™è¯¯: Checkpoint ä¸­æ‰¾ä¸åˆ° 'visual_fusion'ï¼")
        except Exception as e:
            print(f"Visual Fusion åŠ è½½æŠ¥é”™: {e}")

        try:
            model.itm_head.load_state_dict(checkpoint['itm_head'], strict=True)
            model.itm_head.to(device=DEVICE, dtype=DTYPE)
            print(f"ITM Head åŠ è½½æˆåŠŸ")
        except KeyError:
            print("é”™è¯¯: Checkpoint ä¸­æ‰¾ä¸åˆ° 'itm_head'ï¼")
        
    else:
        print(f"æœªæ‰¾åˆ°æƒé‡æ–‡ä»¶: {CHECKPOINT_PATH}")

    model.eval()

    # data preparation
    print("\nå‡†å¤‡æµ‹è¯•å›¾ç‰‡...")
    img_path = "test7.jpg"
    
    if not os.path.exists(img_path):
        url = "http://images.cocodataset.org/val2017/000000039769.jpg"
        print(f"   æœ¬åœ°æ— å›¾ç‰‡ï¼Œæ­£åœ¨ä¸‹è½½ç¤ºä¾‹å›¾ç‰‡: {url}")
        raw_image = Image.open(requests.get(url, stream=True).raw).convert("RGB")
        raw_image.save("test.jpeg")
    else:
        raw_image = Image.open(img_path).convert("RGB")

    # =================================================
    # Task 1: è‡ªå›å½’æ–‡æœ¬ç”Ÿæˆ
    # =================================================
    print("\n" + "="*40)
    print("æµ‹è¯• 1: è‡ªå›å½’æ–‡æœ¬ç”Ÿæˆ")
    print("="*40)
    
    prompt = "Describe this image in detail."
    inputs_gen = processor(images=raw_image, text=prompt, return_tensors="pt").to(DEVICE)
    inputs_gen["pixel_values"] = inputs_gen["pixel_values"].to(dtype=DTYPE)
    
    with torch.no_grad():
        outputs = model.generate(**inputs_gen, max_new_tokens=500)
    
    print(f"Prompt: {prompt}")
    print(f"Output: {processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()}")

    # =================================================
    # Task 2: å›¾æ–‡åŒ¹é… (ITM)
    # =================================================
    print("\n" + "="*40)
    print("æµ‹è¯• 2: å›¾æ–‡åŒ¹é… (ITM)")
    print("="*40)
    
    test_texts = [
        "Imagine you are a robot, and the image shows your current perspective. Your task is to get to the bathroom. Tell me if going in this direction will get you to the bathroom.", 
        "A red sports car driving on the highway", 
        "Imagine you are a robot, and the image shows your current perspective. Your task is to get to the living room and find the white chair. Tell me if going in this direction will get you to there." 
    ]
    
    image_inputs = processor(images=raw_image, return_tensors="pt").to(DEVICE)
    pixel_values = image_inputs.pixel_values.to(dtype=DTYPE) 
    
    # ã€ä¿®æ”¹ 5ã€‘è¾“å…¥æ•°æ®å®‰å…¨æ£€æŸ¥
    if torch.isnan(pixel_values).any():
        print("è‡´å‘½é”™è¯¯: è¾“å…¥å›¾åƒ Tensor åŒ…å« NaNï¼")
        return

    text_inputs = qformer_tokenizer(
        test_texts,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=32
    ).to(DEVICE)
    
    pixel_values_expanded = pixel_values.repeat(len(test_texts), 1, 1, 1)

    print("æ­£åœ¨è®¡ç®— Cross-Attention Fusion åŠ ITM åˆ†æ•°...")
    with torch.no_grad():
        logits = model.forward_itm(
            pixel_values=pixel_values_expanded,
            input_ids=text_inputs.input_ids,
            attention_mask=text_inputs.attention_mask
        )
        probs = torch.softmax(logits, dim=1)
    
    print("\nåŒ¹é…ç»“æœ:")
    for i, text in enumerate(test_texts):
        score_match = probs[i][1].item()
        
        # å®‰å…¨å¤„ç†ï¼Œé˜²æ­¢ä¹‹å‰æ²¡æ•è·çš„ NaN å¯¼è‡´ int() æŠ¥é”™
        if np.isnan(score_match):
            bar_len = 0
            score_str = "NaN"
        else:
            bar_len = int(score_match * 20)
            score_str = f"{score_match:.6f}"
            
        bar = "â–ˆ" * bar_len + "â–‘" * (20 - bar_len)
        print(f"Text: '{text}'")
        print(f"Score: {score_str} | {bar}")
        print("-" * 30)

if __name__ == "__main__":
    run_inference()